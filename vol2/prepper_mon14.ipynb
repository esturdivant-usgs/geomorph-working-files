{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process input data for coastal variable extraction\n",
    "\n",
    "Author: Emily Sturdivant; esturdivant@usgs.gov\n",
    "\n",
    "***\n",
    "\n",
    "Pre-process files to be used in extractor.ipynb (Extract barrier island metrics along transects). See the project [README](https://github.com/esturdivant-usgs/BI-geomorph-extraction/blob/master/README.md) and the Methods Report (Zeigler et al., in review). \n",
    "\n",
    "\n",
    "## Pre-processing steps\n",
    "\n",
    "1. Pre-created geomorphic features: dunes, shoreline points, armoring.\n",
    "2. Inlets\n",
    "3. Shoreline\n",
    "4. Transects - extend and sort\n",
    "5. Transects - tidy\n",
    "\n",
    "\n",
    "## Notes:\n",
    "This process requires some manipulation of the spatial layers by the user. When applicable, instructions are described in this file.\n",
    "\n",
    "***\n",
    "\n",
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import arcpy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import core.functions_warcpy as fwa\n",
    "import core.functions as fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "site (options: CapeLookout, Rockaway, Fisherman, Monomoy, ShipShoal, CapeHatteras, Parramore, FireIsland, Assateague, ParkerRiver, Cobb, Cedar, Metompkin, Smith, CoastGuard, RhodeIsland, Wreck, Myrtle, Forsythe, Assawoman):  Monomoy\n",
      "year (options: 2010, 2012, 2014):  2014\n",
      "Path to project directory (e.g. \\\\Mac\u000b",
      "olume\\dir\\FireIsland2014):  ················································\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setvars.py initialized variables.\n",
      "SITE: Monomoy\n",
      "MHW: 0.39\n",
      "MLW: -0.95\n",
      "Max dune crest height: 3\n",
      "Projection code: 26919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from core.setvars import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables\n",
    "\n",
    "Change the filename variables to match your local files. They should be in an Esri file geodatabase named site+year.gdb in your project directory, which will be the value of the variable `home`. \n",
    "\n",
    "Input the site, year, and project directory path. `setvars.py` retrieves the pre-determined values for that site in that year from `configmap.py`. The project directory will be used to set up your workspace. It's hidden for security – sorry! I recommend that you type the path somewhere and paste it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inputs - vector\n",
    "orig_trans = os.path.join(home, 'trans_orig')\n",
    "\n",
    "# Extended transects: NASC transects extended and sorted, ready to be the base geometry for processing\n",
    "extendedTrans = os.path.join(home, 'Monomoy2014_extTrans_null')\n",
    "\n",
    "# Tidied transects: Extended transects without overlapping transects\n",
    "extTrans_tidy = os.path.join(home, 'Monomoy_tidyTrans') \n",
    "\n",
    "# Geomorphology points: positions of indicated geomorphic features\n",
    "ShorelinePts = os.path.join(home, 'Monomoy2014_SLpts')  # shoreline\n",
    "dlPts = os.path.join(home, 'Monomoy2014_DLpts')         # dune toe\n",
    "dhPts = os.path.join(home, 'Monomoy2014_DHpts')         # dune crest\n",
    "\n",
    "# Inlet lines: polyline feature classes delimiting inlet position. Must intersect the full island shoreline\n",
    "inletLines = os.path.join(home, 'Monomoy2014_inletLines')\n",
    "\n",
    "# Full island shoreline: polygon that outlines the island shoreline, MHW on oceanside and MTL on bayside\n",
    "barrierBoundary = os.path.join(home, 'bndpoly_manual_2sl')  \n",
    "\n",
    "# Elevation grid: DEM of island elevation at either 5 m or 1 m resolution\n",
    "elevGrid = os.path.join(home, 'Monomoy2014_DEM_5m')\n",
    "\n",
    "\n",
    "# ---\n",
    "# OPTIONAL - comment out each one that is not available\n",
    "# ---\n",
    "# Study area boundary; manually digitize if the barrier island study area does not end at an inlet.\n",
    "# SA_bounds = os.path.join(home, 'SA_bounds')\n",
    "\n",
    "# Armoring lines: digitize lines of shorefront armoring to be used if dune toe points are not available.\n",
    "# armorLines = os.path.join(home, 'armorLines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapre = '14CNT01'\n",
    "csvpath = os.path.join(proj_dir, 'Input_Data', '{}_morphology'.format(datapre), '{}_morphology.csv'.format(datapre))\n",
    "\n",
    "# state = sitevals['state']\n",
    "dt_fc, dc_fc, sl_fc = fwa.MorphologyCSV_to_FCsByFeature(csvpath, state, proj_code, csv_fill = 999, fc_fill = -99999, csv_epsg=4326)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dunes\n",
    "Display the points and the DEM in a GIS to check for irregularities. For example, if shoreline points representing a distance less than X m are visually offset from the general shoreline, they should likely be removed. Another red flag is when the positions of dlows and dhighs in relation to the shore are illogical, i.e. dune crests are seaward of dune toes. Address these irregularities by manually deleting points. Delete conservatively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Armoring\n",
    "If the dlows do not capture the entire top-of-beach due to atypical formations caused by anthropogenic modification, you may need to digitize the beachfront armoring. The next code block will generate an empty feature class. Refer to the DEM and orthoimagery. If there is no armoring in the study area, continue. If there is armoring, use the Editing toolbar to add lines to the feature class that trace instances of armoring. Common manifestations of what we call armoring are sandfencing and sandbagging and concrete seawalls. \n",
    "\n",
    "If there is no armoring file in the project geodatabase, the extractor script will notify you that it is proceeding without armoring.\n",
    "\n",
    "*__Requires manipulation in GIS__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arcpy.CreateFeatureclass_management(home, os.path.basename(armorLines), 'POLYLINE', spatial_reference=utmSR)\n",
    "print(\"{} created. Now manually digitize the shorefront armoring.\".format(armorLines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inlets\n",
    "We also need to manually digitize inlets if an inlet delineation does not already exist. To do, the code below will produce the feature class. After which, use the Editing toolbar to create a line where the oceanside shore meets a tidal inlet. If the study area includes both sides of an inlet, that inlet will be represented by two lines. The inlet lines are used to define the bounds of the oceanside shore, which is also considered the point where the oceanside shore meets the bayside. Inlet lines must intersect the MHW contour. \n",
    "\n",
    "What do we do when the study area and not an inlet is the end?\n",
    "\n",
    "*__Requires manipulation in GIS__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# manually create lines that correspond to end of land and cross the MHW line (use bndpoly/DEM)\n",
    "arcpy.CreateFeatureclass_management(home, os.path.basename(inletLines), 'POLYLINE', spatial_reference=utmSR)\n",
    "print(\"{} created. Now we'll stop for you to manually create lines at each inlet.\".format(inletLines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shoreline\n",
    "The shoreline is produced through a combination of the DEM and the shoreline points. The first step converts the DEM to both MTL and MHW contour polygons. Those polygons are combined to produce the full shoreline, which is considered to fall at MHW on the oceanside and MTL on the bayside (to include partially submerged wetland).\n",
    "\n",
    "If the study area does not end cleanly at an inlet, create a separate polyline feature class (default name is 'SA_bounds') and add lines that bisect the shoreline; they should look and function like inlet lines. Specify this in the arguments for DEMtoFullShorelinePoly() and CreateShoreBetweenInlets().\n",
    "\n",
    "At some small inlets, channel depth may be above MTL. In this case, the script left to its own devices will leave the MTL contour between the two inlet lines. This can be rectified after processing by deleting the mid-inlet features from the temp file 'shore_2split.' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SA_bounds created. \n",
      "Now we'll stop for you to manually create lines at each inlet.\n"
     ]
    }
   ],
   "source": [
    "# manually create lines that correspond to end of land and cross the MHW line (use bndpoly/DEM)\n",
    "if len(SA_bounds) and not arcpy.Exists(SA_bounds):\n",
    "    arcpy.CreateFeatureclass_management(home, 'SA_bounds', 'POLYLINE', spatial_reference=utmSR)\n",
    "print(\"{} created. \\nNow we'll stop for you to manually create lines at each inlet.\".format(SA_bounds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the MTL contour polygon from the DEM...\n",
      "Aggregation distance: 10\n",
      "Minimum area: 300\n",
      "Minimum hole size: 300\n",
      "Creating the MHW contour polygon from the DEM...\n",
      "Aggregation distance: 10\n",
      "Minimum area: 300\n",
      "Minimum hole size: 300\n",
      "Combining the two polygons...\n",
      "...delineating land between MTL and MHW elevations...\n",
      "...removing the MHW-MTL areas on the oceanside...\n",
      "User input required! Select extra features in bndpoly for deletion.\n",
      "Recommended technique: select the polygon/s to keep and then Switch Selection.\n",
      "\n",
      "Select features from bndpoly that should not be included in the final shoreline polygon. \n"
     ]
    }
   ],
   "source": [
    "bndpoly = fwa.DEMtoFullShorelinePoly(elevGrid, sitevals['MTL'], sitevals['MHW'], inletLines, ShorelinePts)\n",
    "print('Select features from {} that should not be included in the final shoreline polygon. '.format(bndpoly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Requires display in GIS__*\n",
    "\n",
    "User input is required to identify only the areas within the study area and eliminate isolated landmasses that are not. Once the features to delete are selected, either delete in the GIS or run the code below. Make sure the bndpoly variable matches the layer name in the GIS.\n",
    "\n",
    "__Do not...__ select the features in ArcGIS and then run DeleteFeatures in this Notebook Python kernel. That will delete the entire feature class. \n",
    "\n",
    "```\n",
    "arcpy.DeleteFeatures_management(bndpoly)\n",
    "```\n",
    "\n",
    "The next step snaps the boundary polygon to the shoreline points anywhere they don't already match and as long as as they are within 25 m of each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bndpoly = os.path.join(home, 'bndpoly_manual')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: bndpoly_manual_2sl ... Should be in your home geodatabase.\n"
     ]
    }
   ],
   "source": [
    "barrierBoundary = fwa.NewBNDpoly(bndpoly, ShorelinePts, barrierBoundary, '25 METERS', '50 METERS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ShoreBetweenInlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting bndpoly_2sl at inlets...\n",
      "Preserving only those line segments that intersect shoreline points...\n",
      "Dissolving the line to create ShoreBetweenInlets...\n"
     ]
    }
   ],
   "source": [
    "# This step could be moved out of pre-processing and into the extractor because it doesn't require user input.\n",
    "shoreline = fwa.CreateShoreBetweenInlets(barrierBoundary, inletLines, shoreline, \n",
    "                                         ShorelinePts, proj_code, SA_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transects - extend, sort, and tidy\n",
    "\n",
    "Create extendedTrans, which are NASC transects for the study area extended to cover the island, with gaps filled, and sorted in the field sort_ID.\n",
    "\n",
    "#### 1. Extend the transects and use a copy of the lines to fill alongshore gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Need to remove extra transects? 'y' if barrierBoundary should be used to select.  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trans_orig is already projected in UTM.\n",
      "Transects extended.\n",
      "MANUALLY: use groups of existing transects in new FC '\\\\Mac\\stor\\Projects\\DeepDive\\TE_vol2\\Myrtle\\scratch.gdb\\trans_extended_presort' to fill gaps.\n"
     ]
    }
   ],
   "source": [
    "# Initialize temp file names\n",
    "trans_extended = os.path.join(arcpy.env.scratchGDB, 'trans_extended')\n",
    "trans_presort = trans_extended+'_presort'\n",
    "\n",
    "# Delete transects over 200 m outside of the study area.\n",
    "if input(\"Need to remove extra transects? 'y' if barrierBoundary should be used to select. \") == 'y':\n",
    "    fwa.RemoveTransectsOutsideBounds(orig_trans, barrierBoundary)\n",
    "\n",
    "# Extend transects and create blank duplicate to use to fill gaps\n",
    "fwa.ExtendLine(orig_trans, trans_extended, extendlength, proj_code)\n",
    "fwa.CopyAndWipeFC(trans_extended, trans_presort, ['sort_ID'])\n",
    "print(\"MANUALLY: use groups of existing transects in new FC '{}' to fill gaps.\".format(trans_presort))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Requires manipulation in GIS__*\n",
    "\n",
    "1. Edit the trans_presort_temp feature class. __Move and rotate__ groups of transects to fill in gaps that are greater than 50 m alongshore. There is no need to preserve the original transects, but avoid overlapping the transects with each other and with the originals. Do not move any transects slightly. If they are moved, they will not be deleted in the next stage. If you slightly move any, you can either undo or delete that line entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in trans_extended: 51\n",
      "Number of features in trans_extended_presort: 137\n",
      "trans_extended_presort ready for sorting. It should be in your scratch geodatabase.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\\\\\\\Mac\\\\stor\\\\Projects\\\\DeepDive\\\\TE_vol2\\\\Myrtle\\\\scratch.gdb\\\\trans_extended_presort'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwa.RemoveDuplicates(trans_presort, trans_extended, barrierBoundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Sort the transects along the shore\n",
    "Usually if the shoreline curves, we need to identify different groups of transects for sorting. This is because the GIS will not correctly establish the alongshore order by simple ordering from the identified sort_corner. If this is the case, answer __yes__ to the next prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do we need to sort the transects in batches to preserve the order? (y/n)  n\n",
      "Sort corner (LL, LR, UL, UR):  LL\n"
     ]
    }
   ],
   "source": [
    "sort_lines = fwa.SortTransectPrep(spatialref=utmSR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_lines = os.path.join(arcpy.env.scratchGDB, 'sort_lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Requires manipulation in GIS__*\n",
    "\n",
    "The last step generated an empty sort lines feature class if you indicated that transects need to be sorted in batches to preserve the order. Now, the user creates lines that will be used to spatially sort transects in groups. \n",
    "\n",
    "For each group of transects:\n",
    "\n",
    "1. __Create a new line__ in 'sort_lines' that intersects all transects in the group. The transects intersected by the line will be sorted independently before being appended to the preceding groups.  (*__add example figure__*)\n",
    "2. __Assign values__ for the fields 'sort,' 'sort_corner,' and 'reverse.' 'sort' indicates the order in which the line should be used and 'sort_corn' indicates the corner from which to perform the spatial sort ('LL', 'UL', etc.). 'reverse' indicates whether the order should be reversed (roughly equivalent to 'DESCENDING').\n",
    "3. Run the following code to create a new sorted transect file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new feature class extTrans2 to hold sorted transects...\n",
      "Sorting sort lines by field sort...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'Result' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1d1629bbf0d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mextendedTrans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhome\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'extTrans2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfwa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSortTransectsFromSortLines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans_presort\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextendedTrans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtID_fld\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\esturdivant\\code\\bi-transect-extractor\\core\\functions_warcpy.py\u001b[0m in \u001b[0;36mSortTransectsFromSortLines\u001b[1;34m(in_trans, out_trans, sort_lines_or_corner, tID_fld, verbose)\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sorting sort lines by field sort...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m         \u001b[0msort_lines2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSort_management\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msort_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_lines\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sort'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ASCENDING'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"For each line, creating subset of transects and adding them in order to the new FC...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'Result' and 'str'"
     ]
    }
   ],
   "source": [
    "fwa.SortTransectsFromSortLines(trans_presort, extendedTrans, sort_lines, tID_fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The projection of trans_orig was changed. The new file is trans_orig_utm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NAD_1983_UTM_Zone_19N'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arcpy.Describe(orig_trans).spatialReference.name\n",
    "orig_trans = fwa.ReProject(orig_trans, orig_trans+'_utm', proj_code=arcpy.Describe(extendedTrans).spatialReference.factoryCode)\n",
    "arcpy.Describe(orig_trans).spatialReference.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "extendedTrans = os.path.join(home, 'mon_trans')\n",
    "\n",
    "# project originals to UTM if they're not already\n",
    "# orig_trans = fwa.ReProject(orig_trans, orig_trans+'_utm', proj_code=arcpy.Describe(extendedTrans).spatialReference.factoryCode)\n",
    "\n",
    "# If there's no overlap between transect and original transect, set TRANSECTID, TRANSORDER, LRR to fill\n",
    "fldsToWipe = ['TRANSECTID', 'TRANSORDER', 'LRR']\n",
    "\n",
    "# 2. Remove orig transects from manually created transects\n",
    "# If any of the original extended transects (with null values) are still present in trans_presort, delete them.\n",
    "for row in arcpy.da.SearchCursor(orig_trans, ['SHAPE@']):\n",
    "    nasc_line = row[0]\n",
    "    buff = nasc_line.buffer(10)\n",
    "    with arcpy.da.UpdateCursor(extendedTrans, ['SHAPE@', 'TRANSECTID', 'TRANSORDER']) as cursor:\n",
    "        for trow in cursor:\n",
    "            tran = trow[0]\n",
    "#             if not buff.overlaps(tran): # none of the geometries overlap\n",
    "#                 trow[1] = -99\n",
    "#                 trow[2] = -99\n",
    "# #                 trow[3] = -99\n",
    "#                 cursor.updateRow(trow)\n",
    "            if buff.disjoint(tran): # all of them are disjoint even after reprojecting the originals\n",
    "                trow[1] = -99\n",
    "                trow[2] = -99\n",
    "#                 trow[3] = -99\n",
    "                cursor.updateRow(trow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tidy the extended (and sorted) transects to remove overlap\n",
    "\n",
    "*__Requires manipulation in GIS__*\n",
    "\n",
    "Overlapping transects cause problems during conversion to 5-m points and to rasters. We create a separate feature class with the 'tidied' transects, in which the lines don't overlap. This is largely a manually process with the following steps:\n",
    "\n",
    "1. Select transects to be used to split other transects. Prioritize transects that a) were originally from NASC, b) have dune points within 25 m, and c) are oriented perpendicular to shore. (add example figure)\n",
    "2. Use the Copy Features geoprocessing tool to copy only the selected transects into a new feature class. If desired, here is the code that could be used to copy the selected features and clear the selection:\n",
    "```python\n",
    "arcpy.CopyFeatures_management(extendedTrans, overlapTrans_lines)\n",
    "arcpy.SelectLayerByAttribute_management(extendedTrans, \"CLEAR_SELECTION\")\n",
    "```\n",
    "3. Run the code below to split the transects at the selected lines of overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overlapTrans_lines = os.path.join(arcpy.env.scratchGDB, 'overlapTrans_lines')\n",
    "if not arcpy.Exists(overlapTrans_lines):\n",
    "    overlapTrans_lines = input(\"Filename of the feature class of only 'boundary' transects: \")\n",
    "arcpy.Intersect_analysis([extendedTrans, overlapTrans_lines], trans_x,\n",
    "                         'ALL', output_type=\"POINT\")\n",
    "arcpy.SplitLineAtPoint_management(extendedTrans, trans_x, extTrans_tidy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the extraneous segments manually. Recommended:\n",
    "\n",
    "1. Using Select with Line draw a line to the appropriate side of the boundary transects. This will select the line segments that need to be deleted.\n",
    "1. Delete the selected lines.\n",
    "1. Remove any remaining overlaps entirely by hand. Use the Split Line tool in the Editing toolbar to split lines to be shortened at the points of overlap. Then delete the remnant sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join anthro data to transects\n",
    "\n",
    "1. Convert xls spreadsheet to points \n",
    "2. Select the first points along each transects and create new FC\n",
    "3. Spatial Join the new FC to the updated transects \n",
    "    - one to one\n",
    "    - keep all target features\n",
    "    - keep only the ID fields and the three anthro fields (and the transect fields [LRR, etc.]?)\n",
    "    - intersect\n",
    "\n",
    "4. Join the transect values to the pts based on sort_ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Input shapefiles\n",
    "shlpts_shp = os.path.join(proj_dir, 'rock14_shlpts.shp')\n",
    "dlpts_shp = os.path.join(proj_dir, 'rock14_dlowpts.shp')\n",
    "dhpts_shp = os.path.join(proj_dir, 'rock14_dhighpts.shp')\n",
    "trans_shp = os.path.join(proj_dir, 'rock_trans.shp')\n",
    "shoreline_shp = os.path.join(proj_dir, 'rock14_shoreline.shp')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
