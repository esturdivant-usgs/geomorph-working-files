{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract barrier island metrics along transects\n",
    "\n",
    "Author: Emily Sturdivant, esturdivant@usgs.gov\n",
    "\n",
    "***\n",
    "\n",
    "Extract barrier island metrics along transects for Barrier Island Geomorphology Bayesian Network. See the project [README](https://github.com/esturdivant-usgs/BI-geomorph-extraction/blob/master/README.md) and the Methods Report (Zeigler et al., in review). \n",
    "\n",
    "\n",
    "## Pre-requisites:\n",
    "- All the input layers (transects, shoreline, etc.) must be ready. This is performed with the notebook file prepper.ipynb.\n",
    "- The files servars.py and configmap.py may need to be updated for the current dataset.\n",
    "\n",
    "## Notes:\n",
    "- This notebook includes interactive quality checking, which requires the user's attention. For thorough QC'ing, we recommend displaying the layers in ArcGIS, especially to confirm the integrity of values for variables such as distance to inlet (__Dist2Inlet__) and widths of the landmass (__WidthPart__, etc.). \n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import arcpy\n",
    "import pyproj\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "import core.functions_warcpy as fwa\n",
    "import core.functions as fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2019-07-05\n",
      "pandas version: 0.20.1\n",
      "numpy version: 1.11.2\n",
      "matplotlib version: 1.5.3\n",
      "pyproj version: 1.9.5.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Date: {}\".format(datetime.date.today()))\n",
    "# print(os.__version__)\n",
    "# print(sys.__version__)\n",
    "print('pandas version: {}'.format(pd.__version__))\n",
    "print('numpy version: {}'.format(np.__version__))\n",
    "print('matplotlib version: {}'.format(matplotlib.__version__))\n",
    "# print(io.__version__)\n",
    "# print(arcpy.__version__)\n",
    "print('pyproj version: {}'.format(pyproj.__version__))\n",
    "\n",
    "# print(bi_transect_extractor.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables\n",
    "\n",
    "This cell prompts you for the site, year, and project directory path. `setvars.py` retrieves the pre-determined values for that site in that year from `configmap.py`. The project directory will be used to set up your workspace. It's hidden for security – sorry! I recommend that you type the path somewhere and paste it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "site (options: RhodeIsland, CoastGuard, Assateague, Myrtle, Wreck, FireIsland, Smith, Monomoy, Assawoman, Rockaway, Parramore, CapeHatteras, Forsythe, CapeLookout, ParkerRiver, ShipShoal, Cedar, Fisherman, Cobb, Metompkin):  Wreck\n",
      "year (options: 2010, 2012, 2014):  2014\n",
      "Path to project directory (e.g. \\\\Mac\u000b",
      "olume\\dir\\FireIsland2014):  ··········································\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setvars.py initialized variables.\n",
      "SITE: Wreck\n",
      "MHW: 0.34\n",
      "MLW: -0.6\n",
      "Max dune crest height: 2.5\n",
      "Projection code: 26918\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from core.setvars import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the filename variables to match your local files. They should be in an Esri file geodatabase named site+year.gdb in your project directory, which you input above and is the value of the variable `home`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extended transects: NASC transects extended and sorted, ready to be the base geometry for processing\n",
    "extendedTrans = os.path.join(home, 'extTrans')\n",
    "\n",
    "# Tidied transects: Extended transects without overlapping transects\n",
    "extTrans_tidy = os.path.join(home, 'extTrans') \n",
    "\n",
    "# Geomorphology points: positions of indicated geomorphic features\n",
    "ShorelinePts = os.path.join(home, 'Wreck2014_SLpts')  # shoreline\n",
    "dlPts = os.path.join(home, 'Wreck2014_DLpts')         # dune toe\n",
    "dhPts = os.path.join(home, 'Wreck2014_DHpts')         # dune crest\n",
    "\n",
    "# Inlet lines: polyline feature classes delimiting inlet position. Must intersect the full island shoreline\n",
    "inletLines = os.path.join(home, 'Wreck2014_inletLines')\n",
    "\n",
    "# Full island shoreline: polygon that outlines the island shoreline, MHW on oceanside and MTL on bayside\n",
    "barrierBoundary = os.path.join(home, 'Wreck2014_bndpoly_2sl')  \n",
    "\n",
    "# Elevation grid: DEM of island elevation at either 5 m or 1 m resolution\n",
    "elevGrid = os.path.join(home, 'Wreck2014_dem')\n",
    "\n",
    "# ---\n",
    "# OPTIONAL - comment out each one that is not available\n",
    "# ---\n",
    "# \n",
    "# morphdata_prefix = '14CNT01'\n",
    "\n",
    "# Study area boundary; manually digitize if the barrier island study area does not end at an inlet.\n",
    "# SA_bounds = os.path.join(home, 'SA_bounds')\n",
    "\n",
    "# Armoring lines: digitize lines of shorefront armoring to be used if dune toe points are not available.\n",
    "# armorLines = os.path.join(home, 'armorLines')\n",
    "\n",
    "# Extended transects with Construction, Development, and Nourishment coding\n",
    "# tr_w_anthro = os.path.join(home, 'extTrans_wAnthro')\n",
    "\n",
    "# Piping Plover Habitat BN raster layers\n",
    "SubType = os.path.join(home, 'SubType')   # substrate type\n",
    "VegType = os.path.join(home, 'VegType')   # vegetation type\n",
    "VegDens = os.path.join(home, 'VegDen')    # vegetation density\n",
    "GeoSet = os.path.join(home, 'GeoSet')     # geomorphic setting\n",
    "\n",
    "# Derivatives of inputs: They will be generated during process if they are not found. \n",
    "shoreline = os.path.join(home, 'ShoreBetweenInlets')   # oceanside shoreline between inlets; generated from shoreline polygon, inlet lines, and SA bounds\n",
    "slopeGrid = os.path.join(home, 'slope_5m')   # Slope at 5 m resolution; generated from DEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transect-averaged values\n",
    "We work with the shapefile/feature class as a pandas DataFrame as much as possible to speed processing and minimize reliance on the ArcGIS GUI display.\n",
    "\n",
    "1. Add the bearing of each transect line to the attribute table from the LINE_BEARING geometry attribute.\n",
    "1. Create a pandas dataframe from the transects feature class. In the process, remove some of the unnecessary fields. The resulting dataframe is indexed by __sort_ID__ with columns corresponding to the attribute fields in the transects feature class. \n",
    "2. Add __DD_ID__.\n",
    "3. Join the values from the transect file that includes the three anthropologic development fields, __Construction__,  __Development__, and __Nourishment__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding line bearing field to transects.\n",
      "...converting feature class to array...\n",
      "...converting array to dataframe...\n",
      "\n",
      "Header of transects dataframe (rows 1-5 out of 112): \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID_1</th>\n",
       "      <th>Shape</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>BaselineID</th>\n",
       "      <th>TransOrder</th>\n",
       "      <th>ProcTime</th>\n",
       "      <th>Autogen</th>\n",
       "      <th>StartX</th>\n",
       "      <th>StartY</th>\n",
       "      <th>EndX</th>\n",
       "      <th>...</th>\n",
       "      <th>SHAPE_Leng</th>\n",
       "      <th>TransectId</th>\n",
       "      <th>LRR</th>\n",
       "      <th>LR2</th>\n",
       "      <th>LSE</th>\n",
       "      <th>LCI90</th>\n",
       "      <th>sort_ID</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Azimuth</th>\n",
       "      <th>DD_ID</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.893991</td>\n",
       "      <td>150001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.893991</td>\n",
       "      <td>150002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.893991</td>\n",
       "      <td>150003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.893991</td>\n",
       "      <td>150004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.893991</td>\n",
       "      <td>150005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         OBJECTID_1  Shape  OBJECTID  BaselineID  TransOrder  ProcTime  \\\n",
       "sort_ID                                                                  \n",
       "1               NaN    NaN       NaN         NaN         NaN       NaN   \n",
       "2               NaN    NaN       NaN         NaN         NaN       NaN   \n",
       "3               NaN    NaN       NaN         NaN         NaN       NaN   \n",
       "4               NaN    NaN       NaN         NaN         NaN       NaN   \n",
       "5               NaN    NaN       NaN         NaN         NaN       NaN   \n",
       "\n",
       "         Autogen  StartX  StartY  EndX   ...    SHAPE_Leng  TransectId  LRR  \\\n",
       "sort_ID                                  ...                                  \n",
       "1            NaN     NaN     NaN   NaN   ...           NaN         NaN  NaN   \n",
       "2            NaN     NaN     NaN   NaN   ...           NaN         NaN  NaN   \n",
       "3            NaN     NaN     NaN   NaN   ...           NaN         NaN  NaN   \n",
       "4            NaN     NaN     NaN   NaN   ...           NaN         NaN  NaN   \n",
       "5            NaN     NaN     NaN   NaN   ...           NaN         NaN  NaN   \n",
       "\n",
       "         LR2  LSE  LCI90  sort_ID  Shape_Length     Azimuth   DD_ID  \n",
       "sort_ID                                                              \n",
       "1        NaN  NaN    NaN        1           NaN  265.893991  150001  \n",
       "2        NaN  NaN    NaN        2           NaN  265.893991  150002  \n",
       "3        NaN  NaN    NaN        3           NaN  265.893991  150003  \n",
       "4        NaN  NaN    NaN        4           NaN  265.893991  150004  \n",
       "5        NaN  NaN    NaN        5           NaN  265.893991  150005  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add BEARING field to extendedTrans feature class\n",
    "arcpy.AddGeometryAttributes_management (extendedTrans, 'LINE_BEARING')\n",
    "print(\"Adding line bearing field to transects.\")\n",
    "\n",
    "# Copy feature class to dataframe.\n",
    "trans_df = fwa.FCtoDF(extendedTrans, id_fld=tID_fld, extra_fields=extra_fields)\n",
    "trans_df['DD_ID'] = trans_df[tID_fld] + sitevals['id_init_val']\n",
    "trans_df.drop('Azimuth', axis=1, inplace=True, errors='ignore')\n",
    "trans_df.rename_axis({\"BEARING\": \"Azimuth\"}, axis=1, inplace=True)\n",
    "\n",
    "# Get anthro fields and join to DF\n",
    "if 'tr_w_anthro' in locals():\n",
    "    trdf_anthro = fwa.FCtoDF(tr_w_anthro, id_fld=tID_fld, dffields=['Development', 'Nourishment','Construction'])\n",
    "    trans_df = fun.join_columns(trans_df, trdf_anthro) \n",
    "\n",
    "# Save\n",
    "trans_df.to_pickle(os.path.join(scratch_dir, 'trans_df.pkl'))\n",
    "\n",
    "# Display\n",
    "print(\"\\nHeader of transects dataframe (rows 1-5 out of {}): \".format(len(trans_df)))\n",
    "trans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Header of transects dataframe (rows 1-5 out of 112): \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID_1</th>\n",
       "      <th>Shape</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>BaselineID</th>\n",
       "      <th>TransOrder</th>\n",
       "      <th>ProcTime</th>\n",
       "      <th>Autogen</th>\n",
       "      <th>StartX</th>\n",
       "      <th>StartY</th>\n",
       "      <th>EndX</th>\n",
       "      <th>...</th>\n",
       "      <th>LR2</th>\n",
       "      <th>LSE</th>\n",
       "      <th>LCI90</th>\n",
       "      <th>sort_ID</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>Azimuth</th>\n",
       "      <th>DD_ID</th>\n",
       "      <th>Construction</th>\n",
       "      <th>Nourishment</th>\n",
       "      <th>Development</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.893991</td>\n",
       "      <td>150001</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.893991</td>\n",
       "      <td>150002</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.893991</td>\n",
       "      <td>150003</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.893991</td>\n",
       "      <td>150004</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>265.893991</td>\n",
       "      <td>150005</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         OBJECTID_1  Shape  OBJECTID  BaselineID  TransOrder  ProcTime  \\\n",
       "sort_ID                                                                  \n",
       "1               NaN    NaN       NaN         NaN         NaN       NaN   \n",
       "2               NaN    NaN       NaN         NaN         NaN       NaN   \n",
       "3               NaN    NaN       NaN         NaN         NaN       NaN   \n",
       "4               NaN    NaN       NaN         NaN         NaN       NaN   \n",
       "5               NaN    NaN       NaN         NaN         NaN       NaN   \n",
       "\n",
       "         Autogen  StartX  StartY  EndX     ...       LR2  LSE  LCI90  sort_ID  \\\n",
       "sort_ID                                    ...                                  \n",
       "1            NaN     NaN     NaN   NaN     ...       NaN  NaN    NaN        1   \n",
       "2            NaN     NaN     NaN   NaN     ...       NaN  NaN    NaN        2   \n",
       "3            NaN     NaN     NaN   NaN     ...       NaN  NaN    NaN        3   \n",
       "4            NaN     NaN     NaN   NaN     ...       NaN  NaN    NaN        4   \n",
       "5            NaN     NaN     NaN   NaN     ...       NaN  NaN    NaN        5   \n",
       "\n",
       "         Shape_Length     Azimuth   DD_ID  Construction  Nourishment  \\\n",
       "sort_ID                                                                \n",
       "1                 NaN  265.893991  150001           111          111   \n",
       "2                 NaN  265.893991  150002           111          111   \n",
       "3                 NaN  265.893991  150003           111          111   \n",
       "4                 NaN  265.893991  150004           111          111   \n",
       "5                 NaN  265.893991  150005           111          111   \n",
       "\n",
       "         Development  \n",
       "sort_ID               \n",
       "1                111  \n",
       "2                111  \n",
       "3                111  \n",
       "4                111  \n",
       "5                111  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is no development/nourishment/construction on Ship Shoal so the anthro fields receive 111 values\n",
    "trans_df['Construction'] = 111\n",
    "trans_df['Nourishment'] = 111\n",
    "trans_df['Development'] = 111\n",
    "\n",
    "# Save\n",
    "trans_df.to_pickle(os.path.join(scratch_dir, 'trans_df.pkl'))\n",
    "\n",
    "# Display\n",
    "print(\"\\nHeader of transects dataframe (rows 1-5 out of {}): \".format(len(trans_df)))\n",
    "trans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get XY and Z/slope from SL, DH, DL points within 25 m of transects\n",
    "Add to each transect row the positions of the nearest pre-created beach geomorphic features (shoreline, dune toe, and dune crest).\n",
    "\n",
    "#### If needed, convert morphology points stored locally to feature classes for use.\n",
    "After which, view the new feature classes in a GIS. Isolate the points to the region of interest. Quality check them. Then copy them for use with this code, which will require setting the filenames to match those included here or changing the values included here to match the final filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if \"morphdata_prefix\" in locals():\n",
    "    csvpath = os.path.join(proj_dir, 'Input_Data', '{}_morphology'.format(morphdata_prefix), \n",
    "                           '{}_morphology.csv'.format(morphdata_prefix))\n",
    "    dt_fc, dc_fc, sl_fc = fwa.MorphologyCSV_to_FCsByFeature(csvpath, state, proj_code, \n",
    "                                                            csv_fill = 999, fc_fill = -99999, csv_epsg=4326)\n",
    "    print(\"OUTPUT: morphology point feature classes in the scratch gdb. We recommend QC before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shoreline\n",
    "\n",
    "The MHW shoreline easting and northing (__SL_x__, __SL_y__) are the coordinates of the intersection of the oceanside shoreline with the transect. Each transect is assigned the foreshore slope (__Bslope__) from the nearest shoreline point within 25 m. These values are populated for each transect as follows: \n",
    "1. get __SL_x__ and __SL_y__ at the point where the transect crosses the oceanside shoreline; \n",
    "2. find the closest shoreline point to the intersection point (must be within 25 m) and copy the slope value from the point to the transect in the field __Bslope__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Wreck2014_bndpoly_2sl at inlets...\n",
      "Preserving only those line segments that intersect shoreline points...\n",
      "Dissolving the line to create ShoreBetweenInlets...\n",
      "\n",
      "Matching shoreline points to transects...\n",
      "Using field 'slope' as slope.\n",
      "...duration at transect 100: 0:0:28.1 seconds\n",
      "Duration: 0:0:31.3 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SL_x</th>\n",
       "      <th>SL_y</th>\n",
       "      <th>Bslope</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>429505.183743</td>\n",
       "      <td>4.123887e+06</td>\n",
       "      <td>-0.027475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>429533.012555</td>\n",
       "      <td>4.125042e+06</td>\n",
       "      <td>-0.029202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>429521.264024</td>\n",
       "      <td>4.125192e+06</td>\n",
       "      <td>-0.026655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>429241.399274</td>\n",
       "      <td>4.122614e+06</td>\n",
       "      <td>-0.003623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>429454.345069</td>\n",
       "      <td>4.123433e+06</td>\n",
       "      <td>-0.023206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  SL_x          SL_y    Bslope\n",
       "sort_ID                                       \n",
       "47       429505.183743  4.123887e+06 -0.027475\n",
       "70       429533.012555  4.125042e+06 -0.029202\n",
       "73       429521.264024  4.125192e+06 -0.026655\n",
       "22       429241.399274  4.122614e+06 -0.003623\n",
       "38       429454.345069  4.123433e+06 -0.023206"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not arcpy.Exists(inletLines):\n",
    "    # manually create lines that correspond to end of land and cross the MHW line (refer to shoreline polygon)\n",
    "    arcpy.CreateFeatureclass_management(home, os.path.basename(inletLines), 'POLYLINE', spatial_reference=utmSR)\n",
    "    print(\"OUTPUT: {}. Interrupt execution to manually create lines at each inlet.\".format(inletLines))\n",
    "\n",
    "if not arcpy.Exists(shoreline):\n",
    "    if not 'SA_bounds' in locals(): \n",
    "        SA_bounds = ''\n",
    "    shoreline = fwa.CreateShoreBetweenInlets(barrierBoundary, inletLines, shoreline, ShorelinePts, proj_code, SA_bounds)\n",
    "\n",
    "# Get the XY position where transect crosses the oceanside shoreline\n",
    "sl2trans_df, ShorelinePts = fwa.add_shorelinePts2Trans(extendedTrans, ShorelinePts, shoreline, \n",
    "                                         tID_fld, proximity=pt2trans_disttolerance)\n",
    "\n",
    "# Save and print sample\n",
    "sl2trans_df.to_pickle(os.path.join(scratch_dir, 'sl2trans.pkl'))\n",
    "sl2trans_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: Saved inletLines and shoreline shapefiles in the scratch directory.\n"
     ]
    }
   ],
   "source": [
    "# Export the inlet delineation and shoreline polygons to the scratch directory ultimately for publication\n",
    "arcpy.FeatureClassToFeatureClass_conversion(inletLines, scratch_dir,  pts_name.split('_')[0] + '_inletLines.shp')\n",
    "arcpy.FeatureClassToFeatureClass_conversion(barrierBoundary, scratch_dir,  pts_name.split('_')[0] + '_shoreline.shp')\n",
    "print('OUTPUT: Saved inletLines and shoreline shapefiles in the scratch directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...converting feature class to array...\n",
      "...converting array to dataframe...\n",
      "Number of points in dataset: (426, 13)\n",
      "\n",
      "OBJECTID_______________________________1 | 433_________________   No fills_________No nulls\n",
      "Shape............... nan\n",
      "state............... 12\n",
      "seg___________________________________42 | 46__________________   No fills_________No nulls\n",
      "profile________________________________1 | 132_________________   No fills_________No nulls\n",
      "sl_x_________________________-228.504457 | 610.193853__________   No fills_________No nulls\n",
      "ci95_slx_________________________1.9e-05 | 0.114202____________   No fills_________No nulls\n",
      "slope__________________________-0.064033 | -0.003172___________   No fills_________No nulls\n",
      "easting____________________428646.252982 | 429554.465541_______   No fills_________No nulls\n",
      "northing__________________4121681.988456 | 4126592.371989______   No fills_________No nulls\n",
      "\n",
      "WARNING: Field(s) ['MHW', 'start_date', 'end_date'] in dataframe not included in field_defs.\n",
      "Deleted field \"MHW\"\n",
      "Deleted field \"start_date\"\n",
      "Deleted field \"end_date\"\n",
      "\n",
      "OUTPUT: wre14_SLpts.shp in specified scratch_dir.\n",
      "\n",
      "OUTPUT: wre14_SLpts.csv (size: 0.03 MB) in specified scratch_dir.\n"
     ]
    }
   ],
   "source": [
    "# fun.AddGeographicCoordinates(ShorelinePts)\n",
    "\n",
    "# Convert to pandas DF\n",
    "slpts_df = fwa.FCtoDF(ShorelinePts)\n",
    "slpts_df.head()\n",
    "\n",
    "# Report values\n",
    "xmlfile = os.path.join(scratch_dir, pts_name.split('_')[0] + '_SLpts_eainfo.xml')\n",
    "sl_extra_flds = fun.report_fc_values(slpts_df, field_defs, xmlfile)\n",
    "\n",
    "# Delete extra fields from points feature class and dataframe (which will become CSV)\n",
    "if len(sl_extra_flds) > 0:\n",
    "    for fld in sl_extra_flds:\n",
    "        try:\n",
    "            arcpy.DeleteField_management(ShorelinePts, fld)\n",
    "            print('Deleted field \"{}\"'.format(fld))\n",
    "        except:\n",
    "            print('WARNING: Failed to delete field \"{}\"'.format(fld))\n",
    "            pass\n",
    "arcpy.Delete_management(pts_name.split('_')[0] + '_SLpts.shp')\n",
    "arcpy.FeatureClassToFeatureClass_conversion(ShorelinePts, scratch_dir,  pts_name.split('_')[0] + '_SLpts.shp')\n",
    "print(\"\\nOUTPUT: {} in specified scratch_dir.\".format(os.path.basename(pts_name.split('_')[0] + '_SLpts.shp')))\n",
    "\n",
    "# Save CSV in scratch_dir\n",
    "slpts_df.drop(sl_extra_flds, axis=1, inplace=True)\n",
    "csv_fname = os.path.join(scratch_dir, pts_name.split('_')[0] + '_SLpts.csv')\n",
    "slpts_df.to_csv(csv_fname, na_rep=fill, index=False)\n",
    "sz_mb = os.stat(csv_fname).st_size/(1024.0 * 1024.0)\n",
    "print(\"\\nOUTPUT: {} (size: {:.2f} MB) in specified scratch_dir.\".format(os.path.basename(csv_fname), sz_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dune positions along transects\n",
    "\n",
    "__DL_x__, __DL_y__, and __DL_z__ are the easting, northing, and elevation, respectively, of the nearest dune toe point within 25 meters of the transect. __DH_x__, __DH_y__, and __DH_z__ are the easting, northing, and elevation, respectively, of the nearest dune crest point within 25 meters. \n",
    "\n",
    "__DL_snapX__, __DL_snapY__, __DH_snapX__, and __DH_snapY__ are the eastings and northings of the points \"snapped\" to the transect. \"Snapping\" finds the position along the transect nearest to the point, i.e. orthogonal to the transect. These values are used to find the beach width. The elevation values are not snapped; we use the elevation values straight from the original points. \n",
    "\n",
    "These values are populated as follows: \n",
    "\n",
    "1. Find the nearest dune crest/toe point to the transect and proceed if the distance is less than 25 m. If there are no points within 25 m of the transect, populate the row with Null values.\n",
    "2. Get the X, Y, and Z values of the point. \n",
    "3. Find the position along the transect of an orthogonal line drawn to the dune point (__DL_snapX__, __DL_snapY__, __DH_snapX__, and __DH_snapY__). This is considered the 'snapped' XY position and is calculated using the arcpy geometry method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matching dune points with transects:\n",
      "Using field 'dhigh_z' as DH Z field...\n",
      "The projection of Wreck2014_DHpts was changed. The new file is Wreck2014_DHpts_utm.\n",
      "Using field 'dlow_z' as DL Z field...\n",
      "The projection of Wreck2014_DLpts was changed. The new file is Wreck2014_DLpts_utm.\n",
      "Looping through transects and dune points to find nearest point within 25 m...\n",
      "...duration at transect 100: 0:0:46.9 seconds\n",
      "Duration: 0:0:50.5 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DH_x</th>\n",
       "      <th>DH_y</th>\n",
       "      <th>DH_z</th>\n",
       "      <th>DH_snapX</th>\n",
       "      <th>DH_snapY</th>\n",
       "      <th>DL_x</th>\n",
       "      <th>DL_y</th>\n",
       "      <th>DL_z</th>\n",
       "      <th>DL_snapX</th>\n",
       "      <th>DL_snapY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>429356.8601</td>\n",
       "      <td>4.123156e+06</td>\n",
       "      <td>1.874822</td>\n",
       "      <td>429355.591218</td>\n",
       "      <td>4.123174e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>428911.5185</td>\n",
       "      <td>4.121636e+06</td>\n",
       "      <td>1.171066</td>\n",
       "      <td>428911.382477</td>\n",
       "      <td>4.121638e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>429349.7154</td>\n",
       "      <td>4.123126e+06</td>\n",
       "      <td>1.771089</td>\n",
       "      <td>429349.914517</td>\n",
       "      <td>4.123123e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>429379.2125</td>\n",
       "      <td>4.125433e+06</td>\n",
       "      <td>2.477860</td>\n",
       "      <td>429379.267791</td>\n",
       "      <td>4.125432e+06</td>\n",
       "      <td>429400.4340</td>\n",
       "      <td>4.125432e+06</td>\n",
       "      <td>1.581335</td>\n",
       "      <td>429400.301917</td>\n",
       "      <td>4.125434e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>429421.3591</td>\n",
       "      <td>4.124239e+06</td>\n",
       "      <td>2.770072</td>\n",
       "      <td>429421.829627</td>\n",
       "      <td>4.124232e+06</td>\n",
       "      <td>429457.2962</td>\n",
       "      <td>4.124234e+06</td>\n",
       "      <td>1.939687</td>\n",
       "      <td>429457.243046</td>\n",
       "      <td>4.124235e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                DH_x          DH_y      DH_z       DH_snapX      DH_snapY  \\\n",
       "sort_ID                                                                     \n",
       "33       429356.8601  4.123156e+06  1.874822  429355.591218  4.123174e+06   \n",
       "3        428911.5185  4.121636e+06  1.171066  428911.382477  4.121638e+06   \n",
       "32       429349.7154  4.123126e+06  1.771089  429349.914517  4.123123e+06   \n",
       "78       429379.2125  4.125433e+06  2.477860  429379.267791  4.125432e+06   \n",
       "54       429421.3591  4.124239e+06  2.770072  429421.829627  4.124232e+06   \n",
       "\n",
       "                DL_x          DL_y      DL_z       DL_snapX      DL_snapY  \n",
       "sort_ID                                                                    \n",
       "33               NaN           NaN       NaN            NaN           NaN  \n",
       "3                NaN           NaN       NaN            NaN           NaN  \n",
       "32               NaN           NaN       NaN            NaN           NaN  \n",
       "78       429400.4340  4.125432e+06  1.581335  429400.301917  4.125434e+06  \n",
       "54       429457.2962  4.124234e+06  1.939687  429457.243046  4.124235e+06  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe for both dune crest and dune toe positions\n",
    "dune2trans_df, dhPts, dlPts = fwa.find_ClosestPt2Trans_snap(extendedTrans, dhPts, dlPts, trans_df, \n",
    "                                          tID_fld, proximity=pt2trans_disttolerance)\n",
    "\n",
    "# Save and print sample\n",
    "dune2trans_df.to_pickle(os.path.join(scratch_dir, 'dune2trans.pkl'))\n",
    "dune2trans_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...converting feature class to array...\n",
      "...converting array to dataframe...\n",
      "Number of points in dataset: (259, 14)\n",
      "\n",
      "OBJECTID_______________________________1 | 259_________________   No fills_________No nulls\n",
      "Shape............... nan\n",
      "state............... 12\n",
      "seg___________________________________43 | 46__________________   No fills_________No nulls\n",
      "profile________________________________1 | 115_________________   No fills_________No nulls\n",
      "lon___________________________-75.803794 | -75.795206__________   No fills_________No nulls\n",
      "lat____________________________37.248124 | 37.280189___________   No fills_________No nulls\n",
      "easting____________________428714.037402 | 429486.910222_______   No fills_________No nulls\n",
      "northing__________________4122700.852503 | 4126253.19109_______   No fills_________No nulls\n",
      "dlow_x_______________________-168.243777 | 286.456477__________   No fills_________No nulls\n",
      "dlow_z__________________________1.104955 | 2.366051____________   No fills_________No nulls\n",
      "z_error_________________________0.029377 | 0.427808____________   No fills_________No nulls\n",
      "\n",
      "WARNING: Field(s) ['start_date', 'end_date'] in dataframe not included in field_defs.\n",
      "Deleted field \"start_date\"\n",
      "Deleted field \"end_date\"\n",
      "\n",
      "OUTPUT: wre14_DTpts.csv (size: 0.02 MB) in specified scratch_dir.\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas DF\n",
    "dlpts_df = fwa.FCtoDF(dlPts)\n",
    "\n",
    "# Report values\n",
    "xmlfile = os.path.join(scratch_dir, pts_name.split('_')[0] + '_DTpts_eainfo.xml')\n",
    "dl_extra_flds = fun.report_fc_values(dlpts_df, field_defs, xmlfile)\n",
    "\n",
    "# Delete extra fields from points feature class and dataframe (which will become CSV)\n",
    "for fld in dl_extra_flds:\n",
    "    try:\n",
    "        arcpy.DeleteField_management(dlPts, fld)\n",
    "        print('Deleted field \"{}\"'.format(fld))\n",
    "    except:\n",
    "        print('WARNING: Failed to delete field \"{}\"'.format(fld))\n",
    "        pass\n",
    "arcpy.FeatureClassToFeatureClass_conversion(dlPts, scratch_dir,  pts_name.split('_')[0] + '_DTpts.shp')\n",
    "\n",
    "# Save CSV in scratch_dir\n",
    "dlpts_df.drop(dl_extra_flds, axis=1, inplace=True)\n",
    "csv_fname = os.path.join(scratch_dir, pts_name.split('_')[0] + '_DTpts.csv')\n",
    "dlpts_df.to_csv(csv_fname, na_rep=fill, index=False)\n",
    "sz_mb = os.stat(csv_fname).st_size/(1024.0 * 1024.0)\n",
    "print(\"\\nOUTPUT: {} (size: {:.2f} MB) in specified scratch_dir.\".format(os.path.basename(csv_fname), sz_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...converting feature class to array...\n",
      "...converting array to dataframe...\n",
      "Number of points in dataset: (503, 14)\n",
      "\n",
      "OBJECTID_______________________________9 | 511_________________   No fills_________No nulls\n",
      "Shape............... nan\n",
      "state............... 12\n",
      "seg___________________________________42 | 46__________________   No fills_________No nulls\n",
      "profile________________________________1 | 132_________________   No fills_________No nulls\n",
      "lon___________________________-75.803907 | -75.795455__________   No fills_________No nulls\n",
      "lat____________________________37.238288 | 37.283131___________   No fills_________No nulls\n",
      "easting____________________428704.049019 | 429474.66088________   No fills_________No nulls\n",
      "northing__________________4121608.402911 | 4126582.104525______   No fills_________No nulls\n",
      "dhigh_x______________________-385.260023 | 570.230579__________   No fills_________No nulls\n",
      "dhigh_z_________________________0.944773 | 3.359027____________   No fills_________No nulls\n",
      "z_error_________________________0.012436 | 0.480422____________   No fills_________No nulls\n",
      "\n",
      "WARNING: Field(s) ['start_date', 'end_date'] in dataframe not included in field_defs.\n",
      "Deleted field \"start_date\"\n",
      "Deleted field \"end_date\"\n",
      "\n",
      "OUTPUT: wre14_DCpts.csv (size: 0.05 MB) in specified scratch_dir.\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas DF\n",
    "dhpts_df = fwa.FCtoDF(dhPts)\n",
    "\n",
    "# Report values\n",
    "xmlfile = os.path.join(scratch_dir, pts_name.split('_')[0] + '_DCpts_eainfo.xml')\n",
    "dh_extra_flds = fun.report_fc_values(dhpts_df, field_defs, xmlfile)\n",
    "\n",
    "# Delete extra fields from points feature class and dataframe (which will become CSV)\n",
    "for fld in dh_extra_flds:\n",
    "    try:\n",
    "        arcpy.DeleteField_management(dhPts, fld)\n",
    "        print('Deleted field \"{}\"'.format(fld))\n",
    "    except:\n",
    "        print('WARNING: Failed to delete field \"{}\"'.format(fld))\n",
    "        pass\n",
    "arcpy.FeatureClassToFeatureClass_conversion(dhPts, scratch_dir,  pts_name.split('_')[0] + '_DCpts.shp')\n",
    "\n",
    "# Save CSV in scratch_dir\n",
    "dhpts_df.drop(dh_extra_flds, axis=1, inplace=True)\n",
    "csv_fname = os.path.join(scratch_dir, pts_name.split('_')[0] + '_DCpts.csv')\n",
    "dhpts_df.to_csv(csv_fname, na_rep=fill, index=False)\n",
    "sz_mb = os.stat(csv_fname).st_size/(1024.0 * 1024.0)\n",
    "print(\"\\nOUTPUT: {} (size: {:.2f} MB) in specified scratch_dir.\".format(os.path.basename(csv_fname), sz_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Armoring\n",
    "__Arm_x__, __Arm_y__, and __Arm_z__ are the easting, northing, and elevation, respectively, where an artificial structure crosses the transect in the vicinity of the beach. These features are meant to supplement the dune toe data set by providing an upper limit to the beach in areas where dune toe extraction was confounded by the presence of an artificial structure. Values are populated for each transect as follows: \n",
    "\n",
    "1. Get the positions of intersection between the digitized armoring lines and the transects (Intersect tool from the Overlay toolset); \n",
    "2. Extract the elevation value at each intersection point from the DEM (Extract Multi Values to Points tool from Spatial Analyst); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: Wreck2014_dem_5m at 5x5 resolution.\n",
      "\\\\Mac\\stor\\Projects\\DeepDive\\TE_vol2\\Wreck\\Wreck2014.gdb\\armorLines created. If shorefront armoring exists, interrupt execution to manually digitize.\n",
      "\n",
      "Armoring file either missing or empty so we will proceed without armoring data. If shorefront tampering is present at this site, cancel the operations to digitize.\n"
     ]
    }
   ],
   "source": [
    "# Create elevation raster at 5-m resolution if not already\n",
    "elevGrid = fwa.ProcessDEM_2(elevGrid, utmSR)\n",
    "\n",
    "# Armoring line\n",
    "if not arcpy.Exists(armorLines):\n",
    "    arcpy.CreateFeatureclass_management(home, os.path.basename(armorLines), 'POLYLINE', spatial_reference=utmSR)\n",
    "    print(\"{} created. If shorefront armoring exists, interrupt execution to manually digitize.\".format(armorLines))\n",
    "\n",
    "arm2trans_df = fwa.ArmorLineToTrans_PD(extendedTrans, armorLines, sl2trans_df, tID_fld, proj_code, elevGrid)\n",
    "\n",
    "# Save and print sample\n",
    "arm2trans_df.to_pickle(os.path.join(scratch_dir, 'arm2trans.pkl'))\n",
    "try:\n",
    "    arm2trans_df.sample(5)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add all the positions to the trans_df\n",
    "Join the new dataframes to the transect dataframe. Before it performs the join, `join_columns_id_check()` checks the index and the ID field for potential errors such as whether they are the equal and whether there are duplicated IDs or null values in either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load saved dataframes\n",
    "trans_df = pd.read_pickle(os.path.join(scratch_dir, 'trans_df.pkl'))\n",
    "sl2trans_df = pd.read_pickle(os.path.join(scratch_dir, 'sl2trans.pkl'))\n",
    "dune2trans_df = pd.read_pickle(os.path.join(scratch_dir, 'dune2trans.pkl'))\n",
    "arm2trans_df = pd.read_pickle(os.path.join(scratch_dir, 'arm2trans.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID_1</th>\n",
       "      <th>Shape</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>BaselineID</th>\n",
       "      <th>TransOrder</th>\n",
       "      <th>ProcTime</th>\n",
       "      <th>Autogen</th>\n",
       "      <th>StartX</th>\n",
       "      <th>StartY</th>\n",
       "      <th>EndX</th>\n",
       "      <th>...</th>\n",
       "      <th>DH_snapX</th>\n",
       "      <th>DH_snapY</th>\n",
       "      <th>DL_x</th>\n",
       "      <th>DL_y</th>\n",
       "      <th>DL_z</th>\n",
       "      <th>DL_snapX</th>\n",
       "      <th>DL_snapY</th>\n",
       "      <th>Arm_x</th>\n",
       "      <th>Arm_y</th>\n",
       "      <th>Arm_z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1532.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>429358.142015</td>\n",
       "      <td>4.126083e+06</td>\n",
       "      <td>429370.1324</td>\n",
       "      <td>4.126107e+06</td>\n",
       "      <td>1.340095</td>\n",
       "      <td>429371.840001</td>\n",
       "      <td>4.126084e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>429344.444954</td>\n",
       "      <td>4.123073e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1485.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>429440.048427</td>\n",
       "      <td>4.123732e+06</td>\n",
       "      <td>429458.6577</td>\n",
       "      <td>4.123730e+06</td>\n",
       "      <td>1.485990</td>\n",
       "      <td>429458.365661</td>\n",
       "      <td>4.123734e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1542.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>429063.351091</td>\n",
       "      <td>4.126563e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>429176.720187</td>\n",
       "      <td>4.122259e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         OBJECTID_1  Shape  OBJECTID  BaselineID  TransOrder  ProcTime  \\\n",
       "sort_ID                                                                  \n",
       "91              NaN    NaN       NaN         NaN      1532.0       NaN   \n",
       "31              NaN    NaN       NaN         NaN         NaN       NaN   \n",
       "44              NaN    NaN       NaN         NaN      1485.0       NaN   \n",
       "101             NaN    NaN       NaN         NaN      1542.0       NaN   \n",
       "15              NaN    NaN       NaN         NaN         NaN       NaN   \n",
       "\n",
       "         Autogen  StartX  StartY  EndX  ...         DH_snapX      DH_snapY  \\\n",
       "sort_ID                                 ...                                  \n",
       "91           NaN     NaN     NaN   NaN  ...    429358.142015  4.126083e+06   \n",
       "31           NaN     NaN     NaN   NaN  ...    429344.444954  4.123073e+06   \n",
       "44           NaN     NaN     NaN   NaN  ...    429440.048427  4.123732e+06   \n",
       "101          NaN     NaN     NaN   NaN  ...    429063.351091  4.126563e+06   \n",
       "15           NaN     NaN     NaN   NaN  ...    429176.720187  4.122259e+06   \n",
       "\n",
       "                DL_x          DL_y      DL_z       DL_snapX      DL_snapY  \\\n",
       "sort_ID                                                                     \n",
       "91       429370.1324  4.126107e+06  1.340095  429371.840001  4.126084e+06   \n",
       "31               NaN           NaN       NaN            NaN           NaN   \n",
       "44       429458.6577  4.123730e+06  1.485990  429458.365661  4.123734e+06   \n",
       "101              NaN           NaN       NaN            NaN           NaN   \n",
       "15               NaN           NaN       NaN            NaN           NaN   \n",
       "\n",
       "         Arm_x  Arm_y  Arm_z  \n",
       "sort_ID                       \n",
       "91         NaN    NaN    NaN  \n",
       "31         NaN    NaN    NaN  \n",
       "44         NaN    NaN    NaN  \n",
       "101        NaN    NaN    NaN  \n",
       "15         NaN    NaN    NaN  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join positions of shoreline, dune crest, dune toe, armoring\n",
    "trans_df = fun.join_columns_id_check(trans_df, sl2trans_df, tID_fld)\n",
    "trans_df = fun.join_columns_id_check(trans_df, dune2trans_df, tID_fld)\n",
    "trans_df = fun.join_columns_id_check(trans_df, arm2trans_df, tID_fld)\n",
    "\n",
    "# Save and print sample\n",
    "trans_df.to_pickle(os.path.join(scratch_dir, 'trans_df_beachmetrics.pkl'))\n",
    "trans_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for errors\n",
    "*Optional*\n",
    "\n",
    "Display summary stats / histograms and create feature classes. The feature classes display the locations that will be used to calculate beach width. Review the output feature classes in a GIS to validate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEhCAYAAACOZ4wDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxWZBRESi2CyIiEjU/wFQPM/7cTOMuwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18a55400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plots = trans_df.hist(['DH_z', 'DL_z', 'Arm_z'])\n",
    "\n",
    "# Subplot Labels\n",
    "plots[0][0].set_xlabel(\"Elevation (m in NAVD88)\")\n",
    "plots[0][0].set_ylabel(\"Frequency\")\n",
    "plots[0][1].set_xlabel(\"Elevation (m in NAVD88)\")\n",
    "plots[0][1].set_ylabel(\"Frequency\")\n",
    "try:\n",
    "    plots[0][2].set_xlabel(\"Elevation (m in NAVD88)\")\n",
    "    plots[0][2].set_ylabel(\"Frequency\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... converting dataframe to array... \n",
      "... converting array to feature class... \n",
      "\n",
      "OUTPUT: pts2trans_SL in designated scratch geodatabase.\n",
      "... converting dataframe to array... \n",
      "... converting array to feature class... \n",
      "\n",
      "OUTPUT: ptSnap2trans_DH in designated scratch geodatabase.\n",
      "... converting dataframe to array... \n",
      "... converting array to feature class... \n",
      "\n",
      "OUTPUT: ptSnap2trans_DL in designated scratch geodatabase.\n"
     ]
    }
   ],
   "source": [
    "# Convert dataframe to feature class - shoreline points with slope\n",
    "fwa.DFtoFC(sl2trans_df, os.path.join(arcpy.env.workspace, 'pts2trans_SL'), \n",
    "           spatial_ref=utmSR, id_fld=tID_fld, xy=[\"SL_x\", \"SL_y\"], keep_fields=['Bslope'])\n",
    "print('OUTPUT: pts2trans_SL in designated scratch geodatabase.')\n",
    "\n",
    "# Dune crests\n",
    "try:\n",
    "    fwa.DFtoFC(dune2trans_df, os.path.join(arcpy.env.workspace, 'ptSnap2trans_DH'), \n",
    "               spatial_ref=utmSR, id_fld=tID_fld, xy=[\"DH_snapX\", \"DH_snapY\"], keep_fields=['DH_z'])\n",
    "    print('OUTPUT: ptSnap2trans_DH in designated scratch geodatabase.')\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    pass\n",
    "\n",
    "# Dune toes\n",
    "try:\n",
    "    fwa.DFtoFC(dune2trans_df, os.path.join(arcpy.env.workspace, 'ptSnap2trans_DL'), \n",
    "               spatial_ref=utmSR, id_fld=tID_fld, xy=[\"DL_snapX\", \"DL_snapY\"], keep_fields=['DL_z'])\n",
    "    print('OUTPUT: ptSnap2trans_DL in designated scratch geodatabase.')\n",
    "except Exception as err:\n",
    "    print(err)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate upper beach width and height\n",
    "Upper beach width (__uBW__) and upper beach height (__uBH__) are calculated based on the difference in position between two points: the position of MHW along the transect (__SL_x__, __SL_y__) and the dune toe position or equivalent (usually __DL_snapX__, __DL_snapY__).  In some cases, the dune toe is not appropriate to designate the \"top of beach\" so beach width and height are calculated from either the position of the dune toe, the dune crest, or the base of an armoring structure. The dune crest was only considered a possibility if the dune crest elevation (__DH_zMHW__) was less than or equal to `maxDH`. \n",
    "\n",
    "They are calculated as follows: \n",
    "2. Calculate distances from MHW to the position along the transect of the dune toe (__DistDL__), dune crest (__DistDH__), and armoring (__DistArm__). \n",
    "2. Adjust the elevations to MHW, populating fields __DH_zmhw__, __DL_zmhw__, and __Arm_zmhw__. \n",
    "3. Conditionally select the appropriate feature to represent \"top of beach.\" Dune toe is prioritized. If it is not available and __DH_zmhw__ is less than or equal to maxDH, use dune crest. If neither of the dune positions satisfy the conditions and an armoring feature intersects with the transect, use the armoring position. If none of the three are possible, __uBW__ and __uBH__ will be null. \n",
    "4. Copy the distance to shoreline and height above MHW (__Dist--__, __---zmhw__) to __uBW__ and __uBH__, respectively. \n",
    "\n",
    "Notes:\n",
    "- In some morphology datasets, missing elevation values at a point indicate that the point should not be used to measure beach width. In those cases, use the `skip_missing_z` argument to select whether or not to skip these points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load saved dataframe\n",
    "trans_df = pd.read_pickle(os.path.join(scratch_dir, 'trans_df_beachmetrics.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fields uBW and uBH populated with beach width and beach height.\n"
     ]
    }
   ],
   "source": [
    "# Calculate distances from shore to dunes, etc.\n",
    "trans_df  = fwa.calc_BeachWidth_fill(extendedTrans, trans_df, maxDH, tID_fld, \n",
    "                                     sitevals['MHW'], fill, skip_missing_z=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dist2Inlet\n",
    "\n",
    "\n",
    "Distance to nearest tidal inlet (__Dist2Inlet__) is computed as alongshore distance of each sampling transect from the nearest tidal inlet. This distance includes changes in the path of the shoreline instead of simply a Euclidean distance and reflects sediment transport pathways. It is measured using the oceanside shoreline between inlets (ShoreBetweenInlets). \n",
    "\n",
    "Note that the ShoreBetweenInlets feature class must be both 'dissolved' and 'singlepart' so that each feature represents one-and-only-one shoreline that runs the entire distance between two inlets or equivalent. If the shoreline is bounded on both sides by an inlet, measure the distance to both and assign the minimum distance of the two. If the shoreline meets only one inlet (meaning the study area ends before the island ends), use the distance to the only inlet. \n",
    "\n",
    "The process uses the cut, disjoint, and length geometry methods and properties in ArcPy data access module. The function measure_Dist2Inlet() prints a warning when the difference in Dist2Inlet between two consecutive transects is greater than 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAUTION: Large change in Dist2Inlet values between transects 21 (18    1385.530656\n",
      "Name: Dist2Inlet, dtype: float64 m) and 22 (1741.6258124516853 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 29 (26    2253.638688\n",
      "Name: Dist2Inlet, dtype: float64 m) and 30 (3908.435966203492 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 36 (33    4713.21525\n",
      "Name: Dist2Inlet, dtype: float64 m) and 37 (5235.551309803814 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 60 (57    7119.740356\n",
      "Name: Dist2Inlet, dtype: float64 m) and 61 (7478.590160705724 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 88 (85    9773.434006\n",
      "Name: Dist2Inlet, dtype: float64 m) and 89 (10184.566123435816 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 22 (19     1741.625812\n",
      "110    1207.726085\n",
      "Name: Dist2Inlet, dtype: float64 m) and 23 (1265.8558178904646 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 23 (20     1813.347286\n",
      "111    1265.855818\n",
      "Name: Dist2Inlet, dtype: float64 m) and 24 (1323.130705738494 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 24 (21     1906.714044\n",
      "112    1323.130706\n",
      "Name: Dist2Inlet, dtype: float64 m) and 25 (1378.3038768543438 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 25 (22     1999.436054\n",
      "113    1378.303877\n",
      "Name: Dist2Inlet, dtype: float64 m) and 26 (1434.7299178991864 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 26 (23     2094.713178\n",
      "114    1434.729918\n",
      "Name: Dist2Inlet, dtype: float64 m) and 27 (1491.3323272907992 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 27 (24     2150.349466\n",
      "115    1491.332327\n",
      "Name: Dist2Inlet, dtype: float64 m) and 28 (1545.927336028183 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 28 (25     2200.805436\n",
      "116    1545.927336\n",
      "Name: Dist2Inlet, dtype: float64 m) and 29 (1598.2818086517243 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 29 (26     2253.638688\n",
      "117    1598.281809\n",
      "Name: Dist2Inlet, dtype: float64 m) and 30 (1649.8907936467622 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 30 (27     3908.435966\n",
      "118    1649.890794\n",
      "Name: Dist2Inlet, dtype: float64 m) and 31 (1701.2704663887528 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 31 (28     4032.220181\n",
      "119    1701.270466\n",
      "Name: Dist2Inlet, dtype: float64 m) and 32 (1752.5142932931692 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 32 (29     4087.551845\n",
      "120    1752.514293\n",
      "Name: Dist2Inlet, dtype: float64 m) and 33 (1803.6239164441386 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 33 (30     4257.105004\n",
      "121    1803.623916\n",
      "Name: Dist2Inlet, dtype: float64 m) and 34 (1854.858715829634 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 34 (31     4416.243293\n",
      "122    1854.858716\n",
      "Name: Dist2Inlet, dtype: float64 m) and 35 (1906.2703334184482 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 35 (32     4647.509956\n",
      "123    1906.270333\n",
      "Name: Dist2Inlet, dtype: float64 m) and 36 (1957.370257184207 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 36 (33     4713.215250\n",
      "124    1957.370257\n",
      "Name: Dist2Inlet, dtype: float64 m) and 37 (2008.470143845324 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 37 (34     5235.551310\n",
      "125    2008.470144\n",
      "Name: Dist2Inlet, dtype: float64 m) and 38 (2060.7302379852526 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 38 (35     5338.015895\n",
      "126    2060.730238\n",
      "Name: Dist2Inlet, dtype: float64 m) and 39 (2111.793472861736 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 39 (36     5415.873500\n",
      "127    2111.793473\n",
      "Name: Dist2Inlet, dtype: float64 m) and 40 (2165.6488311591975 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 40 (37     5488.029717\n",
      "128    2165.648831\n",
      "Name: Dist2Inlet, dtype: float64 m) and 41 (2216.703280803554 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 41 (38     5547.170176\n",
      "129    2216.703281\n",
      "Name: Dist2Inlet, dtype: float64 m) and 42 (2267.623830216682 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 42 (39     5614.239807\n",
      "130    2267.623830\n",
      "Name: Dist2Inlet, dtype: float64 m) and 43 (2318.4499223316434 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 43 (40     5689.271297\n",
      "131    2318.449922\n",
      "Name: Dist2Inlet, dtype: float64 m) and 44 (2369.1387999509925 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 44 (41     5770.614793\n",
      "132    2369.138800\n",
      "Name: Dist2Inlet, dtype: float64 m) and 45 (2419.8893754554947 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 45 (42     5832.480944\n",
      "133    2419.889375\n",
      "Name: Dist2Inlet, dtype: float64 m) and 46 (2470.640659081801 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 46 (43     5894.879142\n",
      "134    2470.640659\n",
      "Name: Dist2Inlet, dtype: float64 m) and 47 (2521.2378690979554 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 47 (44     5952.137844\n",
      "135    2521.237869\n",
      "Name: Dist2Inlet, dtype: float64 m) and 48 (2571.950666099969 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 48 (45     6030.458895\n",
      "136    2571.950666\n",
      "Name: Dist2Inlet, dtype: float64 m) and 49 (2622.7581026091107 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 49 (46     6104.632445\n",
      "137    2622.758103\n",
      "Name: Dist2Inlet, dtype: float64 m) and 50 (2622.592682914686 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 50 (47     6156.994536\n",
      "138    2622.592683\n",
      "Name: Dist2Inlet, dtype: float64 m) and 51 (2571.58774444091 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 51 (48     6210.496871\n",
      "139    2571.587744\n",
      "Name: Dist2Inlet, dtype: float64 m) and 52 (2520.621821747328 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 52 (49     6266.616328\n",
      "140    2520.621822\n",
      "Name: Dist2Inlet, dtype: float64 m) and 53 (2469.8401844421987 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 53 (50     6467.236381\n",
      "141    2469.840184\n",
      "Name: Dist2Inlet, dtype: float64 m) and 54 (2419.1644732056716 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 54 (51     6683.363439\n",
      "142    2419.164473\n",
      "Name: Dist2Inlet, dtype: float64 m) and 55 (2368.5401849674245 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 55 (52     6779.688833\n",
      "143    2368.540185\n",
      "Name: Dist2Inlet, dtype: float64 m) and 56 (2318.043569655938 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 56 (53     6843.56860\n",
      "144    2318.04357\n",
      "Name: Dist2Inlet, dtype: float64 m) and 57 (2267.7190001342688 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 57 (54     6911.679516\n",
      "145    2267.719000\n",
      "Name: Dist2Inlet, dtype: float64 m) and 58 (2217.532153677478 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 58 (55     7000.719680\n",
      "146    2217.532154\n",
      "Name: Dist2Inlet, dtype: float64 m) and 59 (2167.4362939596895 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 59 (56     7059.963722\n",
      "147    2167.436294\n",
      "Name: Dist2Inlet, dtype: float64 m) and 60 (2117.366650031758 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 60 (57     7119.740356\n",
      "148    2117.366650\n",
      "Name: Dist2Inlet, dtype: float64 m) and 61 (2067.3317097972986 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 61 (58     7478.590161\n",
      "149    2067.331710\n",
      "Name: Dist2Inlet, dtype: float64 m) and 62 (2017.2951168982067 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 62 (59     7544.640061\n",
      "150    2017.295117\n",
      "Name: Dist2Inlet, dtype: float64 m) and 63 (1967.2596845215573 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 63 (60     7609.655475\n",
      "151    1967.259685\n",
      "Name: Dist2Inlet, dtype: float64 m) and 64 (1917.218470412637 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 64 (61     7753.549019\n",
      "152    1917.218470\n",
      "Name: Dist2Inlet, dtype: float64 m) and 65 (1867.1599169867295 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 65 (62     7831.442055\n",
      "153    1867.159917\n",
      "Name: Dist2Inlet, dtype: float64 m) and 66 (1817.0754341105753 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 66 (63     7888.316275\n",
      "154    1817.075434\n",
      "Name: Dist2Inlet, dtype: float64 m) and 67 (1767.0437221259679 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 67 (64     7940.103527\n",
      "155    1767.043722\n",
      "Name: Dist2Inlet, dtype: float64 m) and 68 (1717.0266587983767 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 68 (65     7993.793548\n",
      "156    1717.026659\n",
      "Name: Dist2Inlet, dtype: float64 m) and 69 (1667.0221723303266 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 69 (66     8047.229944\n",
      "157    1667.022172\n",
      "Name: Dist2Inlet, dtype: float64 m) and 70 (1617.0200451264088 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 70 (67     8101.459283\n",
      "158    1617.020045\n",
      "Name: Dist2Inlet, dtype: float64 m) and 71 (1567.0138184596628 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 71 (68     8288.675744\n",
      "159    1567.013818\n",
      "Name: Dist2Inlet, dtype: float64 m) and 72 (1517.0072517211008 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 72 (69     8355.295106\n",
      "160    1517.007252\n",
      "Name: Dist2Inlet, dtype: float64 m) and 73 (1466.9885981507512 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 73 (70     8588.030989\n",
      "161    1466.988598\n",
      "Name: Dist2Inlet, dtype: float64 m) and 74 (1416.9835111735122 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 74 (71     8651.517917\n",
      "162    1416.983511\n",
      "Name: Dist2Inlet, dtype: float64 m) and 75 (1366.9326603143722 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 75 (72     8740.621195\n",
      "163    1366.932660\n",
      "Name: Dist2Inlet, dtype: float64 m) and 76 (1316.9229019368402 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 76 (73     8804.621303\n",
      "164    1316.922902\n",
      "Name: Dist2Inlet, dtype: float64 m) and 77 (1266.8891245163716 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 77 (74     8876.806678\n",
      "165    1266.889125\n",
      "Name: Dist2Inlet, dtype: float64 m) and 78 (1216.8640906740472 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 78 (75     8930.552840\n",
      "166    1216.864091\n",
      "Name: Dist2Inlet, dtype: float64 m) and 79 (1166.843887531087 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 79 (76     9047.071162\n",
      "167    1166.843888\n",
      "Name: Dist2Inlet, dtype: float64 m) and 80 (1116.7205009894685 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 80 (77     9098.857228\n",
      "168    1116.720501\n",
      "Name: Dist2Inlet, dtype: float64 m) and 81 (1066.5896093413412 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 81 (78     9159.374426\n",
      "169    1066.589609\n",
      "Name: Dist2Inlet, dtype: float64 m) and 82 (1016.4780263023237 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 82 (79     9239.769583\n",
      "170    1016.478026\n",
      "Name: Dist2Inlet, dtype: float64 m) and 83 (966.3775363974033 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 83 (80     9296.061955\n",
      "171     966.377536\n",
      "Name: Dist2Inlet, dtype: float64 m) and 84 (916.2595556161543 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 84 (81     9351.758801\n",
      "172     916.259556\n",
      "Name: Dist2Inlet, dtype: float64 m) and 85 (866.1195542835362 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 85 (82     9485.068375\n",
      "173     866.119554\n",
      "Name: Dist2Inlet, dtype: float64 m) and 86 (816.0012792757445 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 86 (83     9610.284454\n",
      "174     816.001279\n",
      "Name: Dist2Inlet, dtype: float64 m) and 87 (765.959093999609 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 87 (84     9714.884291\n",
      "175     765.959094\n",
      "Name: Dist2Inlet, dtype: float64 m) and 88 (715.9368224877664 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 88 (85     9773.434006\n",
      "176     715.936822\n",
      "Name: Dist2Inlet, dtype: float64 m) and 89 (665.8492477121671 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 89 (86     10184.566123\n",
      "177      665.849248\n",
      "Name: Dist2Inlet, dtype: float64 m) and 90 (615.5230370783748 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 90 (87     10237.025774\n",
      "178      615.523037\n",
      "Name: Dist2Inlet, dtype: float64 m) and 91 (565.0147487507079 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 91 (88     10289.745470\n",
      "179      565.014749\n",
      "Name: Dist2Inlet, dtype: float64 m) and 92 (514.1267444623429 m).\n",
      "CAUTION: Large change in Dist2Inlet values between transects 92 (89     10355.755994\n",
      "180      514.126744\n",
      "Name: Dist2Inlet, dtype: float64 m) and 93 (463.3275460904846 m).\n",
      "Duration: 0:0:4.1 seconds\n",
      "There are duplicate values in the index (\"sort_ID\").\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "There are errors in the index ('sort_ID') and the identified ID column ('sort_ID') does not exist. Errors could include duplicate, null, or fill values in the index.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-9d4de076213b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Join to transects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrans_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_columns_id_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrans_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDist2Inlet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtID_fld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Save and view last 10 rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\esturdivant\\code\\bi-transect-extractor\\core\\functions.py\u001b[0m in \u001b[0;36mjoin_columns_id_check\u001b[1;34m(df1, df2, id_fld, how, fill)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;34m'SplitSort'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Should this be hard-coded as 'SplitSort'? Should it be id_fld instead?\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mcheck_id_fld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_fld\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mcheck_id_fld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_fld\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# remove matching columns from target dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\esturdivant\\code\\bi-transect-extractor\\core\\functions.py\u001b[0m in \u001b[0;36mcheck_id_fld\u001b[1;34m(df, id_fld, fill)\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_fld\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mbad_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\"There are errors in the index ('{}') and the identified ID column ('{}') does not exist. Errors could include duplicate, null, or fill values in the index.\"\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_fld\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mid_fld\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: There are errors in the index ('sort_ID') and the identified ID column ('sort_ID') does not exist. Errors could include duplicate, null, or fill values in the index."
     ]
    }
   ],
   "source": [
    "# Calc Dist2Inlet in new dataframe \n",
    "dist_df = fwa.measure_Dist2Inlet(shoreline, extendedTrans, inletLines, tID_fld)\n",
    "\n",
    "# Join to transects\n",
    "trans_df = fun.join_columns_id_check(trans_df, pd.DataFrame(dist_df.Dist2Inlet), tID_fld, fill=fill)\n",
    "\n",
    "# Save and view last 10 rows\n",
    "dist_df.to_pickle(os.path.join(scratch_dir, 'dist2inlet_df.pkl'))\n",
    "dist_df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip transects, get barrier widths\n",
    "Calculates __WidthLand__, __WidthFull__, and __WidthPart__, which measure different flavors of the cross-shore width of the barrier island. __WidthLand__ is the above-water distance between the back-barrier and seaward MHW shorelines. __WidthLand__ only includes regions of the barrier within the shoreline polygon (bndpoly_2sl) and does not extend into any of the sinuous or intervening back-barrier waterways and islands. __WidthFull__ is the total distance between the back-barrier and seaward MHW shorelines (including space occupied by waterways). __WidthPart__ is the width of only the most seaward portion of land within the shoreline. \n",
    "\n",
    "These are calculated as follows: \n",
    "\n",
    "1. Clip the transect to the full island shoreline (Clip in the Analysis toolbox); \n",
    "2. For __WidthLand__, get the length of the multipart line segment from &quot;SHAPE@LENGTH&quot; feature class attribute. When the feature is multipart, this will include only the remaining portions of the transect; \n",
    "3. For __WidthPart__, convert the clipped transect from multipart to singlepart and get the length of the first line segment, which should be the most seaward; \n",
    "4. For __WidthFull__, calculate the distance between the first vertex and the last vertex of the clipped transect (Feature Class to NumPy Array with explode to points, pandas groupby, numpy hypot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipping the transects to the barrier island boundaries ('clip2island')...\n",
      "Getting the width along each transect of the oceanside land (WidthPart)...\n",
      "...converting feature class to array...\n",
      "...converting array to dataframe...\n",
      "Getting the width along each transect of the entire barrier (WidthFull)...\n",
      "Converting feature class vertices to array with X and Y...\n",
      "...converting array to dataframe...\n",
      "Getting the width along each transect of above water portion of the barrier (WidthLand)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID_1</th>\n",
       "      <th>Shape</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>BaselineID</th>\n",
       "      <th>TransOrder</th>\n",
       "      <th>ProcTime</th>\n",
       "      <th>Autogen</th>\n",
       "      <th>StartX</th>\n",
       "      <th>StartY</th>\n",
       "      <th>EndX</th>\n",
       "      <th>...</th>\n",
       "      <th>Arm_zmhw</th>\n",
       "      <th>DistDL</th>\n",
       "      <th>DistDH</th>\n",
       "      <th>DistArm</th>\n",
       "      <th>uBW</th>\n",
       "      <th>uBH</th>\n",
       "      <th>ub_feat</th>\n",
       "      <th>WidthFull</th>\n",
       "      <th>WidthLand</th>\n",
       "      <th>WidthPart</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sort_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.589978</td>\n",
       "      <td>100.064136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.589978</td>\n",
       "      <td>1.171808</td>\n",
       "      <td>DL</td>\n",
       "      <td>994.351123</td>\n",
       "      <td>984.846161</td>\n",
       "      <td>662.273537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1553.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.671317</td>\n",
       "      <td>63.357016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.671317</td>\n",
       "      <td>1.960618</td>\n",
       "      <td>DL</td>\n",
       "      <td>1029.267173</td>\n",
       "      <td>941.229190</td>\n",
       "      <td>894.513904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1534.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.181123</td>\n",
       "      <td>56.759654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.181123</td>\n",
       "      <td>0.846183</td>\n",
       "      <td>DL</td>\n",
       "      <td>587.671624</td>\n",
       "      <td>587.671624</td>\n",
       "      <td>587.671624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1497.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.844351</td>\n",
       "      <td>121.572100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.844351</td>\n",
       "      <td>1.605969</td>\n",
       "      <td>DL</td>\n",
       "      <td>668.867175</td>\n",
       "      <td>668.867175</td>\n",
       "      <td>668.867175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         OBJECTID_1  Shape  OBJECTID  BaselineID  TransOrder  ProcTime  \\\n",
       "sort_ID                                                                  \n",
       "48              NaN    NaN       NaN         NaN      1489.0       NaN   \n",
       "112             NaN    NaN       NaN         NaN      1553.0       NaN   \n",
       "36              NaN    NaN       NaN         NaN         NaN       NaN   \n",
       "93              NaN    NaN       NaN         NaN      1534.0       NaN   \n",
       "56              NaN    NaN       NaN         NaN      1497.0       NaN   \n",
       "\n",
       "         Autogen  StartX  StartY  EndX     ...      Arm_zmhw     DistDL  \\\n",
       "sort_ID                                    ...                            \n",
       "48           NaN     NaN     NaN   NaN     ...           NaN  33.589978   \n",
       "112          NaN     NaN     NaN   NaN     ...           NaN        NaN   \n",
       "36           NaN     NaN     NaN   NaN     ...           NaN  50.671317   \n",
       "93           NaN     NaN     NaN   NaN     ...           NaN  33.181123   \n",
       "56           NaN     NaN     NaN   NaN     ...           NaN  93.844351   \n",
       "\n",
       "             DistDH  DistArm        uBW       uBH  ub_feat    WidthFull  \\\n",
       "sort_ID                                                                   \n",
       "48       100.064136      NaN  33.589978  1.171808       DL   994.351123   \n",
       "112             NaN      NaN        NaN       NaN      NaN          NaN   \n",
       "36        63.357016      NaN  50.671317  1.960618       DL  1029.267173   \n",
       "93        56.759654      NaN  33.181123  0.846183       DL   587.671624   \n",
       "56       121.572100      NaN  93.844351  1.605969       DL   668.867175   \n",
       "\n",
       "          WidthLand   WidthPart  \n",
       "sort_ID                          \n",
       "48       984.846161  662.273537  \n",
       "112             NaN         NaN  \n",
       "36       941.229190  894.513904  \n",
       "93       587.671624  587.671624  \n",
       "56       668.867175  668.867175  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clip transects, get barrier widths\n",
    "widths_df = fwa.calc_IslandWidths(extendedTrans, barrierBoundary, tID_fld=tID_fld)\n",
    "\n",
    "# # Save\n",
    "widths_df.to_pickle(os.path.join(scratch_dir, 'widths_df.pkl'))\n",
    "\n",
    "# Join\n",
    "trans_df = fun.join_columns_id_check(trans_df, widths_df, tID_fld, fill=fill)\n",
    "\n",
    "# Save\n",
    "trans_df.to_pickle(os.path.join(scratch_dir, trans_name+'_null_prePts.pkl'))\n",
    "trans_df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-m Points\n",
    "The point dataset samples the land every 5 m along each shore-normal transect. \n",
    "\n",
    "### Split transects into points at 5-m intervals. \n",
    "\n",
    "The point dataset is created from the tidied transects (tidyTrans, created during pre-processing) as follows: \n",
    "\n",
    "1. Clip the tidied transects (tidyTrans) to the shoreline polygon (bndpoly_2sl) , retaining only those portions of the transects that represent land.\n",
    "2. Produce a dataframe of point positions along each transect every 5 m starting from the ocean-side shoreline. This uses the positionAlongLine geometry method accessed with a Search Cursor and saves the outputs in a new dataframe. \n",
    "3. Create a point feature class from the dataframe. \n",
    "\n",
    "Note: Sometimes the system doesn't seem to register the new feature class (transPts_unsorted) for a while. I'm not sure how to work around that, other than just to wait. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipping transects to within the shoreline bounds ('tidytrans_clipped')...\n",
      "Getting points every 5m along each transect and saving in new dataframe...\n",
      "Converting dataframe to feature class ('transPts_unsorted')...\n",
      "... converting dataframe to array... \n",
      "... converting array to feature class... \n",
      "\n",
      "Duration: 0:0:34.5 seconds\n",
      "OUTPUT: 'transPts_unsorted' in scratch geodatabase.\n"
     ]
    }
   ],
   "source": [
    "pts_df, pts_presort = fwa.TransectsToPointsDF(extTrans_tidy, barrierBoundary, fc_out=pts_presort)\n",
    "print(\"OUTPUT: '{}' in scratch geodatabase.\".format(os.path.basename(pts_presort)))\n",
    "\n",
    "# Save\n",
    "pts_df.to_pickle(os.path.join(scratch_dir, 'pts_presort.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Elevation and Slope to points\n",
    "\n",
    "__ptZ__ (later __ptZmhw__) and __ptSlp__ are the elevation and slope at the 5-m cell corresponding to the point. \n",
    "1. Create the slope and DEM rasters if they don't already exist. We use the 5-m DEM to generate a slope surface (Slope tool in 3D Analyst). \n",
    "2. Use Extract Multi Values to Points tool in Spatial Analyst. \n",
    "3. Convert the feature class back to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ExecuteError",
     "evalue": "Failed to execute. Parameters are not valid.\nERROR 000865: Input raster: \\\\Mac\\stor\\Projects\\DeepDive\\TE_vol2\\Wreck\\Wreck2014.gdb\\Wreck2014_dem_5m does not exist.\nFailed to execute (Slope).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExecuteError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d9bfd642dd81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create slope raster from DEM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslopeGrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0marcpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSlope_3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melevGrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslopeGrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PERCENT_RISE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"OUTPUT: slope file in designated home geodatabase.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\ddd.py\u001b[0m in \u001b[0;36mSlope\u001b[1;34m(in_raster, out_raster, output_measurement, z_factor, method, z_unit)\u001b[0m\n\u001b[0;32m   5250\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5251\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5252\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\ddd.py\u001b[0m in \u001b[0;36mSlope\u001b[1;34m(in_raster, out_raster, output_measurement, z_factor, method, z_unit)\u001b[0m\n\u001b[0;32m   5247\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0marcpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marcobjects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marcobjectconversion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconvertArcObjectToPythonObject\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5248\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5249\u001b[1;33m         \u001b[0mretval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvertArcObjectToPythonObject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSlope_3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgp_fixargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_raster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_raster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_measurement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_unit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5250\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5251\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ArcGIS\\Pro\\Resources\\ArcPy\\arcpy\\geoprocessing\\_base.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    504\u001b[0m         \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mgp_fixargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mconvertArcObjectToPythonObject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mExecuteError\u001b[0m: Failed to execute. Parameters are not valid.\nERROR 000865: Input raster: \\\\Mac\\stor\\Projects\\DeepDive\\TE_vol2\\Wreck\\Wreck2014.gdb\\Wreck2014_dem_5m does not exist.\nFailed to execute (Slope).\n"
     ]
    }
   ],
   "source": [
    "# Create slope raster from DEM\n",
    "if not arcpy.Exists(slopeGrid):\n",
    "    arcpy.Slope_3d(elevGrid, slopeGrid, 'PERCENT_RISE')\n",
    "    print(\"OUTPUT: slope file in designated home geodatabase.\")\n",
    "    \n",
    "# Add elevation and slope values at points.\n",
    "arcpy.sa.ExtractMultiValuesToPoints(pts_presort, [[elevGrid, 'ptZ'], [slopeGrid, 'ptSlp']])\n",
    "print(\"OUTPUT: added slope and elevation to '{}' in designated scratch geodatabase.\".format(os.path.basename(pts_presort)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'SubType' in locals():\n",
    "    # Add substrate type, geomorphic setting, veg type, veg density values at points.\n",
    "    arcpy.sa.ExtractMultiValuesToPoints(pts_presort, [[SubType, 'SubType'], [VegType, 'VegType'], \n",
    "                                                           [VegDens, 'VegDens'], [GeoSet, 'GeoSet']])\n",
    "\n",
    "    # Convert to dataframe\n",
    "    pts_df = fwa.FCtoDF(pts_presort, xy=True, dffields=[tID_fld,'ptZ', 'ptSlp', 'SubType', \n",
    "                                                             'VegType', 'VegDens', 'GeoSet'])\n",
    "    # Recode fill values\n",
    "    pts_df.replace({'GeoSet': {9999:np.nan}, 'SubType': {9999:np.nan}, 'VegType': {9999:np.nan},\n",
    "                    'VegDens': {9999:np.nan}}, inplace=True)\n",
    "else:\n",
    "    print(\"Plover BN layers not specified (we only check for SubType), so we'll proceed without them. \")\n",
    "    # Convert to dataframe\n",
    "    pts_df = fwa.FCtoDF(pts_presort, xy=True, dffields=[tID_fld,'ptZ', 'ptSlp'])\n",
    "\n",
    "# Save and view sample\n",
    "pts_df.to_pickle(os.path.join(scratch_dir, 'pts_extractedvalues_presort.pkl'))\n",
    "pts_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print histogram of elevation extracted to points\n",
    "plots = pts_df.hist('ptZ')\n",
    "\n",
    "# Subplot Labels\n",
    "plots[0][0].set_xlabel(\"Elevation (m in NAVD88)\")\n",
    "plots[0][0].set_ylabel(\"Frequency\")\n",
    "\n",
    "# Display\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate distances and sort points\n",
    "\n",
    "__SplitSort__ is a unique numeric identifier of the 5-m points at the study site, sorted by order along shoreline and by distance from oceanside. __SplitSort__ values are populated by sorting the points by __sort_ID__ and __Dist_Seg__ (see below). \n",
    "\n",
    "__Dist_Seg__ is the Euclidean distance between the point and the seaward shoreline (__SL_x__, __SL_y__). __Dist_MHWbay__ is the distance between the point and the bayside shoreline and is calculated by subtracting the __Dist_Seg__ value from the __WidthPart__ value of the transect. \n",
    "\n",
    "__DistSegDH__, __DistSegDL__, and __DistSegArm__ measure the distance of each 5-m point from the dune crest and dune toe position along a particular transect. They are calculated as the Euclidean distance between the 5-m point and the given feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load saved dataframes\n",
    "pts_df = pd.read_pickle(os.path.join(scratch_dir, 'pts_extractedvalues_presort.pkl'))\n",
    "trans_df = pd.read_pickle(os.path.join(scratch_dir, trans_name+'_null_prePts.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate DistSeg, Dist_MHWbay, DistSegDH, DistSegDL, DistSegArm, and sort points (SplitSort)\n",
    "pts_df = fun.join_columns(pts_df, trans_df, tID_fld)\n",
    "pts_df = fun.prep_points(pts_df, tID_fld, pID_fld, sitevals['MHW'], fill)\n",
    "\n",
    "# Aggregate ptZmhw to max and mean and join to transects\n",
    "pts_df, zmhw = fun.aggregate_z(pts_df, sitevals['MHW'], tID_fld, 'ptZ', fill)\n",
    "trans_df = fun.join_columns(trans_df, zmhw) \n",
    "\n",
    "# Join transect values to pts\n",
    "pts_df = fun.join_columns(pts_df, trans_df, tID_fld)\n",
    "\n",
    "# pID_fld needs to be among the columns\n",
    "if not pID_fld in pts_df.columns:\n",
    "    pts_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "# Match field names to those in sorted_pt_flds list\n",
    "for fld in pts_df.columns:\n",
    "    if fld not in sorted_pt_flds:\n",
    "        for i, fldi in enumerate(sorted_pt_flds):\n",
    "            if fldi.lower() == fld.lower():\n",
    "                sorted_pt_flds[i] = fld   \n",
    "                print(fld)\n",
    "    \n",
    "# Drop extra fields and sort columns\n",
    "trans_df.drop(extra_fields, axis=1, inplace=True, errors='ignore')\n",
    "for i, f in enumerate(sorted_pt_flds):\n",
    "    for c in pts_df.columns:\n",
    "        if f.lower() == c.lower():\n",
    "            sorted_pt_flds[i] = c\n",
    "pts_df = pts_df.reindex_axis(sorted_pt_flds, axis=1)\n",
    "\n",
    "# Save dataframes \n",
    "trans_df.to_pickle(os.path.join(scratch_dir, trans_name+'_null.pkl'))\n",
    "pts_df.to_pickle(os.path.join(scratch_dir, pts_name+'_null.pkl'))\n",
    "\n",
    "# View random rows from the points DF\n",
    "pts_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use pyproj to convert projected coordinates to geographic coordinates (lat, lon in NAD83)\n",
    "utm = pyproj.Proj(init='epsg:{}'.format(proj_code))\n",
    "nad = pyproj.Proj(init='epsg:4269') # NAD83\n",
    "\n",
    "in_y = pts_df['seg_y'].tolist()\n",
    "in_x = pts_df['seg_x'].tolist()\n",
    "\n",
    "lon, lat = pyproj.transform(utm, nad, in_x,in_y)\n",
    "\n",
    "lon_col = 'seg_lon'\n",
    "lat_col = 'seg_lat'\n",
    "\n",
    "pts_df[lon_col] = lon\n",
    "pts_df[lat_col] = lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recode the values for CSV output and model running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recode\n",
    "pts_df4csv = pts_df.replace({'SubType': {7777:'{1111, 2222}', 1000:'{1111, 3333}'}, \n",
    "                              'VegType': {77:'{11, 22}', 88:'{22, 33}', 99:'{33, 44}'},\n",
    "                              'VegDens': {666: '{111, 222}', 777: '{222, 333}', \n",
    "                                          888: '{333, 444}', 999: '{222, 333, 444}'}})\n",
    "\n",
    "# Fill NAs\n",
    "pts_df4csv.fillna(fill, inplace=True) \n",
    "\n",
    "# Save and view sample\n",
    "pts_df4csv.to_pickle(os.path.join(scratch_dir, pts_name+'_csv.pkl'))\n",
    "pts_df4csv.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality checking\n",
    "Look at extracted profiles from around the island. Enter the transect ID within the available range when prompted. Evaluate the plots for consistency among variables. Repeat various times until you can be satisfied that the variables are consistent with each other and appear to represent reality. View areas with inconsistencies in a GIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "desccols = ['DL_zmhw', 'DH_zmhw', 'Arm_zmhw', 'uBW', 'uBH', 'Dist2Inlet', \n",
    "            'WidthPart', 'WidthLand', 'WidthFull', 'mean_Zmhw', 'max_Zmhw']\n",
    "\n",
    "# Histograms\n",
    "trans_df.hist(desccols, sharey=True, figsize=[15, 10], bins=20)\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flds_dist = ['SplitSort', 'Dist_Seg', 'Dist_MHWbay', 'DistSegDH', 'DistSegDL', 'DistSegArm']\n",
    "flds_z = ['ptZmhw', 'ptZ', 'ptSlp']\n",
    "pts_df.loc[:,flds_dist+flds_z].describe()\n",
    "pts_df.hist(flds_dist, sharey=True, figsize=[15, 8], layout=(2,3))\n",
    "pts_df.hist(flds_z, sharey=True, figsize=[15, 4], layout=(1,3))\n",
    "\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prompt for transect identifier (sort_ID) and get all points from that transect.\n",
    "trans_in = int(input('Transect ID (\"sort_ID\" {:d}-{:d}): '.format(int(pts_df[tID_fld].head(1)), int(pts_df[tID_fld].tail(1)))))\n",
    "pts_set = pts_df[pts_df[tID_fld] == trans_in]\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(13,10))\n",
    "\n",
    "# Plot the width of the island.\n",
    "ax1 = fig.add_subplot(211)\n",
    "try:\n",
    "    fun.plot_island_profile(ax1, pts_set, sitevals['MHW'], sitevals['MTL'])\n",
    "except TypeError as err:\n",
    "    print('TypeError: {}'.format(err))\n",
    "    pass\n",
    "\n",
    "# Zoom in on the upper beach.\n",
    "ax2 = fig.add_subplot(212)\n",
    "try:\n",
    "    fun.plot_beach_profile(ax2, pts_set, sitevals['MHW'], sitevals['MTL'], maxDH)\n",
    "except TypeError as err:\n",
    "    print('TypeError: {}'.format(err))\n",
    "    pass \n",
    "\n",
    "# Display\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report field values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dataframe\n",
    "pts_df4csv = pd.read_pickle(os.path.join(scratch_dir, pts_name+'_csv.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xmlfile = os.path.join(scratch_dir, pts_name+'_eainfo.xml')\n",
    "fun.report_fc_values(pts_df4csv, field_defs, xmlfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "### Transect-averaged\n",
    "Output the transect-averaged metrics in the following formats:\n",
    "- transects, unpopulated except for ID values, as gdb feature class\n",
    "- transects, unpopulated except for ID values, as shapefile\n",
    "- populated transects with fill values as gdb feature class\n",
    "- populated transects with null values as gdb feature class\n",
    "- populated transects with fill values as shapefile\n",
    "- raster of beach width (__uBW__) by transect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "trans_df = pd.read_pickle(os.path.join(scratch_dir, trans_name+'_null.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create transect file with only ID values and geometry to publish.\n",
    "trans_flds = ['TRANSECTID', 'TRANSORDER', 'DD_ID']\n",
    "for i, f in enumerate(trans_flds):\n",
    "    for c in trans_df.columns:\n",
    "        if f.lower() == c.lower():\n",
    "            trans_flds[i] = c\n",
    "            \n",
    "trans_4pub = fwa.JoinDFtoFC(trans_df.loc[:,trans_flds], extendedTrans, tID_fld, out_fc=sitevals['code']+'_trans')\n",
    "out_shp = arcpy.FeatureClassToFeatureClass_conversion(trans_4pub, scratch_dir, sitevals['code']+'_trans.shp')\n",
    "print(\"OUTPUT: {} in specified scratch_dir.\".format(os.path.basename(str(out_shp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trans_4pubdf = fwa.FCtoDF(trans_4pub)\n",
    "xmlfile = os.path.join(scratch_dir, trans_4pub + '_eainfo.xml')\n",
    "trans_df_extra_flds = fun.report_fc_values(trans_4pubdf, field_defs, xmlfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create transect FC with fill values - Join values from trans_df to the transect FC as a new file.\n",
    "trans_fc = fwa.JoinDFtoFC(trans_df, extendedTrans, tID_fld, out_fc=trans_name+'_fill')\n",
    "\n",
    "# Create transect FC with null values\n",
    "fwa.CopyFCandReplaceValues(trans_fc, fill, None, out_fc=trans_name+'_null', out_dir=home)\n",
    "\n",
    "# Save final transect SHP with fill values\n",
    "out_shp = arcpy.FeatureClassToFeatureClass_conversion(trans_fc, scratch_dir, trans_name+'_shp.shp')\n",
    "print(\"OUTPUT: {} in specified scratch_dir.\".format(os.path.basename(str(out_shp))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raster - beach width\n",
    "It may be necessary to close any Arc sessions you have open."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a template raster corresponding to the transects. \n",
    "if not arcpy.Exists(rst_transID):\n",
    "    print(\"{} was not found so we will create the base raster.\".format(os.path.basename(rst_transID)))\n",
    "    outEucAll = arcpy.sa.EucAllocation(extTrans_tidy, maximum_distance=50, cell_size=cell_size, source_field=tID_fld)\n",
    "    outEucAll.save(os.path.basename(rst_transID))\n",
    "\n",
    "# Create raster of uBW values by joining trans_df to the template raster.\n",
    "out_rst = fwa.JoinDFtoRaster(trans_df, os.path.basename(rst_transID), bw_rst, fill, tID_fld, 'uBW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-m points\n",
    "\n",
    "Output the point metrics in the following formats:\n",
    "- tabular, in CSV\n",
    "- populated points with fill values as gdb feature class\n",
    "- populated points with null values as gdb feature class\n",
    "- populated points with fill values as shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the saved dataframes\n",
    "pts_df4csv = pd.read_pickle(os.path.join(scratch_dir, pts_name+'_csv.pkl'))\n",
    "pts_df = pd.read_pickle(os.path.join(scratch_dir, pts_name+'_null.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save CSV in scratch_dir\n",
    "csv_fname = os.path.join(scratch_dir, pts_name +'.csv')\n",
    "pts_df4csv.to_csv(csv_fname, na_rep=fill, index=False)\n",
    "\n",
    "sz_mb = os.stat(csv_fname).st_size/(1024.0 * 1024.0)\n",
    "print(\"\\nOUTPUT: {} (size: {:.2f} MB) in specified scratch_dir.\".format(os.path.basename(csv_fname), sz_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert pts_df to FC - automatically converts NaNs to fills (default fill is -99999)\n",
    "pts_fc = fwa.DFtoFC_large(pts_df, out_fc=os.path.join(arcpy.env.workspace, pts_name+'_fill'), \n",
    "                          spatial_ref=utmSR, df_id=pID_fld, xy=[\"seg_x\", \"seg_y\"])\n",
    "\n",
    "# Save final FCs with null values\n",
    "fwa.CopyFCandReplaceValues(pts_fc, fill, None, out_fc=pts_name+'_null', out_dir=home)\n",
    "\n",
    "# Save final points as SHP with fill values\n",
    "out_pts_shp = arcpy.FeatureClassToFeatureClass_conversion(pts_fc, scratch_dir, pts_name+'_shp.shp')\n",
    "print(\"OUTPUT: {} in specified scratch_dir.\".format(os.path.basename(str(out_pts_shp))))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
